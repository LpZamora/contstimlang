{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/columbiadpml/anaconda3/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/columbiadpml/anaconda3/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/columbiadpml/anaconda3/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from lstm_class import RNNLM\n",
    "from bilstm_class import RNNLM_bilstm\n",
    "from rnn_class import RNNModel\n",
    "from model_functions import model_factory\n",
    "\n",
    "\n",
    "#model names\n",
    "model1_name='lstm'\n",
    "model2_name='xlm'\n",
    "\n",
    "#bigram and trigram models run on CPU, so gpu_id will be ignored\n",
    "model1_gpu_id=0\n",
    "model2_gpu_id=0\n",
    "\n",
    "#squashing thresholds for two models\n",
    "squash_threshold1=50 \n",
    "squash_threshold2=50 \n",
    "\n",
    "#sentence length\n",
    "sent_len=8\n",
    "\n",
    "\n",
    "model1=model_factory(model1_name,model1_gpu_id)\n",
    "model2=model_factory(model2_name,model2_gpu_id)\n",
    "\n",
    "get_model1_sent_prob=model1.sent_prob\n",
    "get_model2_sent_prob=model2.sent_prob\n",
    "get_model1_word_probs=model1.word_probs\n",
    "get_model2_word_probs=model2.word_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('vocab_low.pkl', 'rb') as file:\n",
    "    vocab_low=pickle.load(file) \n",
    "    \n",
    "with open('vocab_low_freqs.pkl', 'rb') as file:\n",
    "    vocab_low_freqs=pickle.load(file) \n",
    "\n",
    "with open('vocab_cap.pkl', 'rb') as file:\n",
    "    vocab_cap=pickle.load(file) \n",
    "    \n",
    "with open('vocab_cap_freqs.pkl', 'rb') as file:\n",
    "    vocab_cap_freqs=pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(prob,squash_threshold):\n",
    "    prob=10*np.log(1+math.e**((prob+squash_threshold)/10))-squash_threshold\n",
    "    return prob\n",
    "\n",
    "\n",
    "def cont_score(model1_sent1_prob,model1_sent2_prob,model2_sent1_prob,model2_sent2_prob):\n",
    "    \n",
    "    gamma = 100 # subject noise\n",
    "\n",
    "    model1_sent1_prob=squash(np.log(model1_sent1_prob),squash_threshold1)\n",
    "    model1_sent2_prob=squash(np.log(model1_sent2_prob),squash_threshold1)\n",
    "    model2_sent1_prob=squash(np.log(model2_sent1_prob),squash_threshold2)\n",
    "    model2_sent2_prob=squash(np.log(model2_sent2_prob),squash_threshold2)\n",
    "    \n",
    "    s1a_b = model1_sent1_prob - model1_sent2_prob\n",
    "    s2a_b = model2_sent1_prob - model2_sent2_prob\n",
    "            \n",
    "\n",
    "    # Prob that model 1 is better and a subject picks a/b\n",
    "    p1a = (1/2) / (1 + np.exp(-(s1a_b)/gamma))\n",
    "    p1b = 1/2 - p1a\n",
    "\n",
    "    # Prob that model 2 is better and a subject picks a/b\n",
    "    p2a = (1/2) / (1 + np.exp(-(s2a_b)/gamma))\n",
    "    p2b = 1/2 - p2a\n",
    "\n",
    "\n",
    "    # Mutual information of model and sentence pick\n",
    "    # Each term is p(model,sent)*log(p(model,sent)/(p(model)*p(sent)))\n",
    "    conta = p1a * np.log2(p1a/((p1a+p2a)/2)) + \\\n",
    "             p2a * np.log2(p2a/((p1a+p2a)/2)) \n",
    "    \n",
    "    contb = p1b * np.log2(p1b/((p1b+p2b)/2)) + \\\n",
    "             p2b * np.log2(p2b/((p1b+p2b)/2))\n",
    "    \n",
    "    return conta,contb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0473041780204074e-96, 2.4398614576462305e-30, 1.0473041780204074e-96, 2.1491492289890032e-30]\n",
      "Know it still seen done secret in come\n",
      "Know it still seen done secret in come\n",
      "\n",
      "\n",
      "[1.9423680982566405e-98, 6.741196340096998e-35, 1.0473041780204074e-96, 2.0784549199942183e-30]\n",
      "Know it RESTARTS seen done secret in come\n",
      "Know it STILL seen done secret in come\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordi=np.arange(sent_len)\n",
    "wordis=[]  \n",
    "for i in range(1000):\n",
    "    random.shuffle(wordi)\n",
    "    wordis=wordis+list(wordi)\n",
    "     \n",
    "words1=list(np.random.choice(vocab_cap, 1, p=vocab_cap_freqs)) + list(np.random.choice(vocab_low, sent_len-1, p=vocab_low_freqs, replace=False))\n",
    "words2=words1.copy()\n",
    "\n",
    "\n",
    "words1o=words1.copy()\n",
    "words2o=words2.copy()\n",
    "\n",
    "sent1=' '.join(words1)\n",
    "sent2=' '.join(words2)\n",
    "\n",
    "\n",
    "model1_sent1_prob=get_model1_sent_prob(sent1)\n",
    "model2_sent1_prob=get_model2_sent_prob(sent1)\n",
    "\n",
    "model1_sent2_prob=get_model1_sent_prob(sent2)\n",
    "model2_sent2_prob=get_model2_sent_prob(sent2)\n",
    "\n",
    "\n",
    "conta_last,contb_last=cont_score(model1_sent1_prob,model1_sent2_prob,model2_sent1_prob,model2_sent2_prob) \n",
    "\n",
    "\n",
    "prob_diff_last12=np.log(model1_sent1_prob/model2_sent1_prob)\n",
    "prob_diff_last21=np.log(model2_sent2_prob/model1_sent2_prob)\n",
    "\n",
    "probs_all=[model1_sent1_prob,model2_sent1_prob,model1_sent2_prob,model2_sent2_prob]\n",
    "\n",
    "probs_all_list=[]\n",
    "print(probs_all)\n",
    "print(sent1)\n",
    "print(sent2)\n",
    "print('\\n')\n",
    "    \n",
    "\n",
    "\n",
    "for samp in range(10000):\n",
    "    \n",
    "    words1o=words1.copy()\n",
    "    words2o=words2.copy()\n",
    "\n",
    "    wordi=int(wordis[samp])\n",
    "    \n",
    "    cur_word1=words1[wordi]\n",
    "    cur_word2=words2[wordi]\n",
    "    \n",
    "    if wordi==0:\n",
    "        vocab=vocab_cap\n",
    "    else:\n",
    "        vocab=vocab_low\n",
    "    \n",
    "    \n",
    "    model1_word1_probs = get_model1_word_probs(words1,wordi)\n",
    "    model1_word2_probs = get_model1_word_probs(words2,wordi)\n",
    "\n",
    "    \n",
    "    if len(model1_word1_probs)==2:\n",
    "        model1_word1_inds=model1_word1_probs[1]\n",
    "        model1_word1_probs=model1_word1_probs[0]\n",
    "        model1_word2_inds=model1_word2_probs[1]\n",
    "        model1_word2_probs=model1_word2_probs[0]\n",
    "    else:\n",
    "        model1_word1_inds=np.arange(len(vocab))\n",
    "        model1_word2_inds=np.arange(len(vocab))\n",
    "          \n",
    "    words1=words1o.copy()\n",
    "    words2=words2o.copy()\n",
    "    \n",
    "    model2_word1_probs = get_model2_word_probs(words1,wordi)\n",
    "    model2_word2_probs = get_model2_word_probs(words2,wordi)\n",
    "    \n",
    "\n",
    "    if len(model2_word1_probs)==2:\n",
    "        model2_word1_inds=model2_word1_probs[1]\n",
    "        model2_word1_probs=model2_word1_probs[0]\n",
    "        model2_word2_inds=model2_word2_probs[1]\n",
    "        model2_word2_probs=model2_word2_probs[0]\n",
    "    else:\n",
    "        model2_word1_inds=np.arange(len(vocab))\n",
    "        model2_word2_inds=np.arange(len(vocab))\n",
    "\n",
    "    word1_inds=list(set(model1_word1_inds)&set(model2_word1_inds))\n",
    "    word2_inds=list(set(model1_word2_inds)&set(model2_word2_inds))\n",
    "\n",
    "    model1_word1_probs=[model1_word1_probs[wi] for wi,i in enumerate(word1_inds) if i in model1_word1_inds]\n",
    "    model1_word2_probs=[model1_word2_probs[wi] for wi,i in enumerate(word2_inds) if i in model1_word2_inds]\n",
    "    model2_word1_probs=[model2_word1_probs[wi] for wi,i in enumerate(word1_inds) if i in model2_word1_inds]\n",
    "    model2_word2_probs=[model2_word2_probs[wi] for wi,i in enumerate(word2_inds) if i in model2_word2_inds]\n",
    "    \n",
    "    \n",
    "    word1_list=[vocab[w] for w in word1_inds]\n",
    "    word2_list=[vocab[w] for w in word2_inds]\n",
    "    \n",
    "    \n",
    "        \n",
    "    model1_word1_probs=model1_word1_probs/np.sum(model1_word1_probs)\n",
    "    model1_word2_probs=model1_word2_probs/np.sum(model1_word2_probs)\n",
    "    \n",
    "    model2_word1_probs=model2_word1_probs/np.sum(model2_word1_probs)\n",
    "    model2_word2_probs=model2_word2_probs/np.sum(model2_word2_probs)\n",
    "    \n",
    "    word1_probs12=model1_word1_probs*np.log(model1_word1_probs/model2_word1_probs)\n",
    "    word2_probs21=model2_word2_probs*np.log(model2_word2_probs/model1_word2_probs)\n",
    "\n",
    "    word1_probs12[np.isnan(word1_probs12)]=0\n",
    "    word1_probs12[np.where(word1_probs12<0)[0]]=0\n",
    "    word1_probs12=word1_probs12/np.sum(word1_probs12)\n",
    "    \n",
    "    word2_probs21[np.isnan(word2_probs21)]=0\n",
    "    word2_probs21[np.where(word2_probs21<0)[0]]=0\n",
    "    word2_probs21=word2_probs21/np.sum(word2_probs21)\n",
    "\n",
    "    word1_tops=[word1_list[vp] for vp in np.argsort(word1_probs12)[::-1][:10]] + [cur_word1]\n",
    "    word2_tops=[word2_list[vp] for vp in np.argsort(word2_probs21)[::-1][:10]] + [cur_word2]\n",
    "\n",
    "    model1_sent1_probs=[]\n",
    "    model1_sent2_probs=[]\n",
    "    model2_sent1_probs=[]\n",
    "    model2_sent2_probs=[]\n",
    "    \n",
    "    sent1_conts12=[]\n",
    "    sent2_conts21=[]\n",
    "    \n",
    "    for word1,word2 in zip(word1_tops,word2_tops):\n",
    "        \n",
    "        words1t=words1o.copy()\n",
    "        words2t=words2o.copy()\n",
    "        \n",
    "        words1t[wordi]=word1\n",
    "        words2t[wordi]=word2\n",
    "        \n",
    "        sent1t=' '.join(words1t)\n",
    "        sent2t=' '.join(words2t)\n",
    "\n",
    "        model1_sent1t_prob=get_model1_sent_prob(sent1t)\n",
    "        model2_sent1t_prob=get_model2_sent_prob(sent1t)\n",
    "        \n",
    "        model1_sent2t_prob=get_model1_sent_prob(sent2t)\n",
    "        model2_sent2t_prob=get_model2_sent_prob(sent2t)\n",
    "        \n",
    "        model1_sent1_probs.append(model1_sent1t_prob)\n",
    "        model1_sent2_probs.append(model1_sent2t_prob)\n",
    "    \n",
    "        model2_sent1_probs.append(model2_sent1t_prob)\n",
    "        model2_sent2_probs.append(model2_sent2t_prob)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    conta_scores=[]\n",
    "    contb_scores=[]\n",
    "    cont_score_inds=[]\n",
    "    a=-1  \n",
    "    for m11,m21 in zip(model1_sent1_probs,model2_sent1_probs):\n",
    "        a+=1\n",
    "        b=-1\n",
    "        for m12,m22 in zip(model1_sent2_probs,model2_sent2_probs):\n",
    "            b+=1\n",
    "            \n",
    "            conta,contb = cont_score(m11,m12,m21,m22) \n",
    "            conta_scores.append(conta)\n",
    "            contb_scores.append(contb)\n",
    "            cont_score_inds.append([a,b])\n",
    "  \n",
    "    aa=cont_score_inds[np.argmax(conta_scores)][0]\n",
    "    bb=cont_score_inds[np.argmax(contb_scores)][1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    new_word1=word1_tops[aa]\n",
    "    new_word2=word2_tops[bb]\n",
    "\n",
    "    new_word1o=new_word1\n",
    "    new_word2o=new_word2\n",
    "\n",
    "    words1[wordi]=new_word1.upper()\n",
    "    words2[wordi]=new_word2.upper()\n",
    "    \n",
    "    sent1p=' '.join(words1)\n",
    "    sent2p=' '.join(words2)\n",
    "\n",
    "    words1[wordi]=new_word1.lower()\n",
    "    if wordi==0 or new_word1o[0].isupper():\n",
    "        words1[wordi]=new_word1.lower().capitalize()\n",
    "\n",
    "    words2[wordi]=new_word2.lower()\n",
    "    if wordi==0 or new_word2o[0].isupper():\n",
    "        words2[wordi]=new_word2.lower().capitalize()\n",
    "    \n",
    "    sent1=' '.join(words1)\n",
    "    sent2=' '.join(words2)\n",
    "\n",
    "    model1_sent1_prob=get_model1_sent_prob(sent1)\n",
    "    model2_sent1_prob=get_model2_sent_prob(sent1)\n",
    "\n",
    "    model1_sent2_prob=get_model1_sent_prob(sent2)\n",
    "    model2_sent2_prob=get_model2_sent_prob(sent2)\n",
    "\n",
    "\n",
    "    probs_all=[model1_sent1_prob,model2_sent1_prob,model1_sent2_prob,model2_sent2_prob]\n",
    "    \n",
    "    probs_all_list.append(probs_all)\n",
    "\n",
    "    print(probs_all)\n",
    "    print(sent1p)\n",
    "    print(sent2p)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_sent1_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('sents10k.txt','r')\n",
    "text=file.read()\n",
    "sents=text.split('.\\n')\n",
    "\n",
    "for si,sent in enumerate(sents):\n",
    "    \n",
    "    words=sent.split(' ')\n",
    "    words[0]=words[0].capitalize()\n",
    "    sent=' '.join(words)\n",
    "    sents[si]=sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=[]\n",
    "for si,s in enumerate(sents):\n",
    "    \n",
    "    if si%100==0:\n",
    "        print(si)\n",
    "    try:\n",
    "        p=get_model2_sent_prob(s+' .')\n",
    "    except:\n",
    "        p=0\n",
    "    \n",
    "    probs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(get_model2_sent_prob('I am going to be one of them .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model2_sent_prob('This is the last time you will hear .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
