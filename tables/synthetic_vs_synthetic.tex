\begin{tabularx}{\textwidth}{lllc}
\toprule
                                                       sentence &                               log probability (model 1) &                               log probability (model 2) &  \# human choices \\
\midrule
          $s_1$: The points are both ignored and poorly argued. &             $\log p(s_1 | \textrm{GPT-2})=$\num{-51.72} &  $\log p(s_1 | \textrm{RoBERTa})=$\textbf{\num{-53.02}} &  \textbf{\num{2}} \\
    $s_2$: The information was very sketch and poorly sketched. &    $\log p(s_2 | \textrm{GPT-2})=$\textbf{\num{-36.62}} &           $\log p(s_2 | \textrm{RoBERTa})=$\num{-65.06} &           \num{0} \\\midrule
                    $s_1$: Try to be selective in what you pre. &           $\log p(s_1 | \textrm{RoBERTa})=$\num{-55.65} &    $\log p(s_1 | \textrm{GPT-2})=$\textbf{\num{-34.40}} &  \textbf{\num{4}} \\
                $s_2$: Enter to be true given that between and. &  $\log p(s_2 | \textrm{RoBERTa})=$\textbf{\num{-48.72}} &             $\log p(s_2 | \textrm{GPT-2})=$\num{-70.65} &           \num{0} \\\midrule
 $s_1$: It experienced strong shearing in the portals exterior. &           $\log p(s_1 | \textrm{ELECTRA})=$\num{-64.98} &     $\log p(s_1 | \textrm{BERT})=$\textbf{\num{-60.43}} &  \textbf{\num{2}} \\
               $s_2$: I had high Fahrenheit in my fifties days. &  $\log p(s_2 | \textrm{ELECTRA})=$\textbf{\num{-45.49}} &              $\log p(s_2 | \textrm{BERT})=$\num{-87.13} &           \num{0} \\\midrule
             $s_1$: There were no christians in the bronze Age. &              $\log p(s_1 | \textrm{BERT})=$\num{-61.96} &  $\log p(s_1 | \textrm{ELECTRA})=$\textbf{\num{-29.62}} &  \textbf{\num{1}} \\
                $s_2$: There is rich finds from the Bronze Age. &     $\log p(s_2 | \textrm{BERT})=$\textbf{\num{-48.46}} &           $\log p(s_2 | \textrm{ELECTRA})=$\num{-44.34} &           \num{0} \\\midrule
                  $s_1$: No wonder the greats try to avoid her. &               $\log p(s_1 | \textrm{XLM})=$\num{-50.19} &    $\log p(s_1 | \textrm{GPT-2})=$\textbf{\num{-34.15}} &  \textbf{\num{1}} \\
                 $s_2$: No doubt this human liked to hurt them. &      $\log p(s_2 | \textrm{XLM})=$\textbf{\num{-42.48}} &             $\log p(s_2 | \textrm{GPT-2})=$\num{-50.95} &           \num{0} \\\midrule
             $s_1$: You may need to remove zipped folder first. &              $\log p(s_1 | \textrm{LSTM})=$\num{-79.48} &    $\log p(s_1 | \textrm{GPT-2})=$\textbf{\num{-36.10}} &  \textbf{\num{1}} \\
         $s_2$: You obviously need to speed someone unless and. &     $\log p(s_2 | \textrm{LSTM})=$\textbf{\num{-52.42}} &             $\log p(s_2 | \textrm{GPT-2})=$\num{-68.22} &           \num{0} \\\midrule
           $s_1$: There are probably two root feelings in this. &               $\log p(s_1 | \textrm{RNN})=$\num{-93.58} &  $\log p(s_1 | \textrm{RoBERTa})=$\textbf{\num{-56.40}} &  \textbf{\num{4}} \\
         $s_2$: There are too great internships Mulder with it. &      $\log p(s_2 | \textrm{RNN})=$\textbf{\num{-48.61}} &           $\log p(s_2 | \textrm{RoBERTa})=$\num{-93.33} &           \num{0} \\\midrule
           $s_1$: That voice made me want to scream repeatedly. &            $\log p(s_1 | \textrm{3-gram})=$\num{-98.04} &      $\log p(s_1 | \textrm{XLM})=$\textbf{\num{-36.90}} &  \textbf{\num{2}} \\
                 $s_2$: I quickly made me want to be unfounded. &   $\log p(s_2 | \textrm{3-gram})=$\textbf{\num{-81.88}} &               $\log p(s_2 | \textrm{XLM})=$\num{-52.35} &           \num{0} \\\midrule
               $s_1$: Verde is the english equivalent of a mit. &           $\log p(s_1 | \textrm{2-gram})=$\num{-142.42} &  $\log p(s_1 | \textrm{ELECTRA})=$\textbf{\num{-46.48}} &  \textbf{\num{1}} \\
      $s_2$: What is severely disabled veterans of text Vishnu. &  $\log p(s_2 | \textrm{2-gram})=$\textbf{\num{-110.48}} &           $\log p(s_2 | \textrm{ELECTRA})=$\num{-83.85} &           \num{0} \\
\bottomrule
\end{tabularx}
