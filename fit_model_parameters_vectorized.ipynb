{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import pickle\n",
    "plt.rcParams['figure.dpi']= 200\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_stim=pd.read_excel('contstim_pilot_n11_spreadsheet.xlsx')\n",
    "task_data=pd.read_csv('data_exp_22452-v9_task-ax2v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['bigram','trigram','rnn','lstm','bilstm','bert','bert_whole_word','roberta','xlm','electra','gpt2']\n",
    "\n",
    "prob_dict=dict()\n",
    "for model_name in models:\n",
    "    f = open(model_name+'_expt1_sentence_probs.pkl','rb')\n",
    "    dict1=pickle.load(f)\n",
    "    prob_dict.update(dict1)\n",
    "    \n",
    "    \n",
    "events=list(task_data['Event Index'])\n",
    "start_inds=[i for i,e in enumerate(events) if e=='1']\n",
    "\n",
    "fitsets=[]\n",
    "\n",
    "for i,start_ind in enumerate(start_inds):\n",
    "\n",
    "    setnum=task_data['counterbalance-o1ql'][start_ind]\n",
    "\n",
    "    if i==12:\n",
    "        setnum='set 12'\n",
    "\n",
    "    sub=task_data['Participant External Session ID'][start_ind]\n",
    "\n",
    "    model1_list=list(task_stim['sentence1_model_'+setnum])[1:]\n",
    "    model2_list=list(task_stim['sentence2_model_'+setnum])[1:]\n",
    "    sent1_list=list(task_stim['sentence1_'+setnum])[1:]\n",
    "    sent2_list=list(task_stim['sentence2_'+setnum])[1:]\n",
    "\n",
    "    if setnum=='set 1':\n",
    "        source_list=list(task_stim['source'])[1:]\n",
    "\n",
    "    else:\n",
    "        source_list=list(task_stim['source_'+setnum])[1:]\n",
    "\n",
    "    if i<len(start_inds)-1:\n",
    "        responses_list=list(task_data['Response'][start_ind:start_inds[i+1]])\n",
    "\n",
    "    else:\n",
    "        responses_list=list(task_data['Response'][start_ind:])\n",
    "\n",
    "    responses_list=[r for r in responses_list if str(r)!='nan']\n",
    "\n",
    "    item_ids=[]\n",
    "\n",
    "    fitset=[]\n",
    "    \n",
    "    for t in range(110):\n",
    "\n",
    "        source=source_list[t]\n",
    "\n",
    "        model1=model1_list[t]\n",
    "        model2=model2_list[t]\n",
    "        sent1=sent1_list[t]\n",
    "        sent2=sent2_list[t]\n",
    "\n",
    "        model1_name=model1[:-2]\n",
    "        model2_name=model2[:-2]        \n",
    "\n",
    "        model_ind=models.index(model1_name)\n",
    "\n",
    "        sents=[sent1,sent2]\n",
    "        sents.sort()\n",
    "        item_id='_'.join(sents)\n",
    "        \n",
    "        if item_id in item_ids:\n",
    "            continue\n",
    "        \n",
    "        item_ids.append(item_id)\n",
    "\n",
    "        response=responses_list[t]\n",
    "\n",
    "        response_ind=[sent1,sent2].index(response)      \n",
    "\n",
    "        log_p1=prob_dict[sent1]\n",
    "        log_p2=prob_dict[sent2]\n",
    "\n",
    "        #if source=='generator':\n",
    "            \n",
    "        fitset.append([model1_name,log_p1,log_p2,response_ind])\n",
    "        \n",
    "    fitsets.append(fitset)\n",
    "\n",
    "fitsets_all=[]\n",
    "for fitset in fitsets:\n",
    "    fitsets_all+=fitset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "minps=[]\n",
    "for m in models:\n",
    "    m=np.min([np.min([trial[1],trial[2]]) for trial in fitsets_all if trial[0]==m])\n",
    "    minps.append(m)\n",
    "    \n",
    "minps=np.abs(minps)\n",
    "\n",
    "\n",
    "maxps=[]\n",
    "for m in models:\n",
    "    m=np.max([np.max([trial[1],trial[2]]) for trial in fitsets_all if trial[0]==m])\n",
    "    maxps.append(m)\n",
    "    \n",
    "maxps=np.abs(maxps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overflow within exponentials when calculating probabilities, we'll use torch.logsumexp where log(exp(x)+1) is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(1+exp(x)) | direct implementation: tensor([ 0.0000, 50.0000,  0.3133,  0.6931,  1.3133, 50.0000,     inf])\n",
      "log(1+exp(x)) | with logsumexp: tensor([  0.0000,  50.0000,   0.3133,   0.6931,   1.3133,  50.0000, 100.0000])\n",
      "log(1/(1+exp(x))) | direct implementation: tensor([  0.0000, -50.0000,  -0.3133,  -0.6931,  -1.3133, -50.0000,     -inf])\n",
      "log(1/(1+exp(x))) | with logsumexp: tensor([  -0.0000,  -50.0000,   -0.3133,   -0.6931,   -1.3133,  -50.0000,\n",
      "        -100.0000])\n",
      "log(1 - 1/(1+exp(x))) | direct implementation: tensor([   -inf,  0.0000, -1.3133, -0.6931, -0.3133,  0.0000,  0.0000])\n",
      "log(1 - 1/(1+exp(x))) | with logsumexp: tensor([-100.0000,    0.0000,   -1.3133,   -0.6931,   -0.3133,    0.0000,\n",
      "           0.0000])\n"
     ]
    }
   ],
   "source": [
    "# this is mathemathically eqivalent to torch.log(1+torch.exp(x)) but it doesn't overflow when x is big.\n",
    "log_1_plus_exp_x=lambda x: torch.logsumexp(torch.stack((x,torch.zeros_like(x)),dim=-1),dim=1)\n",
    "\n",
    "# this is mathemathically equivalent to log(1/(1+exp(x)))\n",
    "log_p=lambda x: -log_1_plus_exp_x(x) \n",
    "\n",
    "# this is mathemathically equivalent to log(1 - 1/(1+exp(x)) )\n",
    "log_1_minus_p=lambda x: x+log_p(x)\n",
    "\n",
    "x=torch.tensor([-100.0,50.0,-1.0,0.0,1.0,50.0,100.0])\n",
    "print('log(1+exp(x)) | direct implementation:', torch.log(1+torch.exp(x)))\n",
    "print('log(1+exp(x)) | with logsumexp:', log_1_plus_exp_x(x))\n",
    "\n",
    "print('log(1/(1+exp(x))) | direct implementation:', torch.log(1/(1+torch.exp(x))))\n",
    "print('log(1/(1+exp(x))) | with logsumexp:', log_p(x))\n",
    "\n",
    "print('log(1 - 1/(1+exp(x))) | direct implementation:', torch.log(1-1/(1+torch.exp(x))))\n",
    "print('log(1 - 1/(1+exp(x))) | with logsumexp:', log_1_minus_p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelToHumanDecision():\n",
    "    def _set_initial_parameters(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __init__(self,model_name_list,parameters=None,device=None):\n",
    "        if device is None:\n",
    "            device='cpu'\n",
    "        \n",
    "        self.device=torch.device(device)\n",
    "        self.model_name_list=model_name_list\n",
    "        self.n_models=len(model_name_list)\n",
    "        \n",
    "        if parameters is None:\n",
    "            self.parameters=self._initial_parameters()\n",
    "        else:\n",
    "            self.parameters={key:torch.tensor(value,device=self.device,dtype=torch.float64) for (key,value) in parameters.items()}\n",
    "\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def decision_NLL(self,log_p1,log_p2,model=None):\n",
    "        \n",
    "        log_p1=torch.as_tensor(log_p1,device=self.device,dtype=torch.float64)\n",
    "        log_p2=torch.as_tensor(log_p2,device=self.device,dtype=torch.float64)\n",
    "        \n",
    "        if model is None: # evaluate all models. # log_p1, log_p2 (n_models,n_sentence_pairs)\n",
    "            assert log_p1.ndim==2 and log_p1.shape[0]==self.n_models and log_p2.ndim==3 and log_p2.shape[0]==self.n_models \n",
    "            slicer=...\n",
    "        elif type(model) is str: # one particular model specified by its name\n",
    "            slicer=self.model_name_list.index(model)\n",
    "        elif type(model) is int: # one particular model specified by its index\n",
    "            slicer=model\n",
    "        elif type(model) is list: # a list of model names\n",
    "            slicer=torch.tensor([self.model_name_list.index(m) for m in model])\n",
    "        else:\n",
    "            raise \n",
    "        cur_parameters={}\n",
    "        for par_name, par in self.parameters.items():\n",
    "            if par.nelement()==1:\n",
    "                cur_parameters[par_name]=par\n",
    "            else:\n",
    "                cur_parameters[par_name]=par[slicer]\n",
    "        return self._f(log_p1,log_p2,cur_parameters)\n",
    "    def get_parameters(self):\n",
    "        numpy_parameters={}\n",
    "        for par_name, par in self.parameters.items():\n",
    "            if par.nelement()==1:\n",
    "                numpy_parameters[par_name]=par.item()\n",
    "            else:\n",
    "                numpy_parameters[par_name]=par.detach().cpu().numpy()\n",
    "        return numpy_parameters\n",
    "\n",
    "class FixedWidthSquashing(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['squashes']=torch.ones(self.n_models,device=self.device)*200.0 # this doesn't work\n",
    "        parameters['gamma']=torch.tensor(10.0,device=self.device)\n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        squash_threshold=cur_parameters['squashes']     \n",
    "        width=1.0\n",
    "        log_p1_corrected=width*log_1_plus_exp_x((log_p1+squash_threshold)/width)-squash_threshold\n",
    "        log_p2_corrected=width*log_1_plus_exp_x((log_p2+squash_threshold)/width)-squash_threshold\n",
    "        s1a_b = log_p1_corrected - log_p2_corrected\n",
    "#         p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "#         p2=1-p1\n",
    "#         choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))        \n",
    "        log_p1=log_p(-s1a_b/gamma)\n",
    "        log_p2=log_1_minus_p(-s1a_b/gamma)\n",
    "        choice_NLL=-torch.stack([log_p1,log_p2],dim=-1)\n",
    "        return choice_NLL\n",
    "\n",
    "    \n",
    "class FixedWidthSquashingVariableGamma(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['squashes']=torch.ones(self.n_models,device=self.device)*200.0 # this doesn't work\n",
    "        parameters['gamma']=torch.ones(self.n_models,device=self.device)*10.0\n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        squash_threshold=cur_parameters['squashes']     \n",
    "        width=1.0\n",
    "        log_p1_corrected=width*log_1_plus_exp_x((log_p1+squash_threshold)/width)-squash_threshold\n",
    "        log_p2_corrected=width*log_1_plus_exp_x((log_p2+squash_threshold)/width)-squash_threshold\n",
    "        #log_p1_corrected=width*torch.log(1+math.e**((log_p1+squash_threshold)/width))-squash_threshold\n",
    "        #log_p2_corrected=width*torch.log(1+math.e**((log_p2+squash_threshold)/width))-squash_threshold\n",
    "        s1a_b = log_p1_corrected - log_p2_corrected\n",
    "#         p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "#         p2=1-p1\n",
    "#         choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))\n",
    "        log_p1=log_p(-s1a_b/gamma)\n",
    "        log_p2=log_1_minus_p(-s1a_b/gamma)\n",
    "        choice_NLL=-torch.stack([log_p1,log_p2],dim=-1)\n",
    "        return choice_NLL\n",
    "\n",
    "class VariableWidthSquashing(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['squashes']=torch.ones(self.n_models,device=self.device)*200.0 # this doesn't work\n",
    "        parameters['gamma']=torch.tensor(10.0,device=self.device)\n",
    "        parameters['width']=torch.ones(self.n_models,device=self.device)\n",
    "        \n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        squash_threshold=cur_parameters['squashes']     \n",
    "        width=torch.nn.Softplus()(cur_parameters['width'])+1e-1\n",
    "        log_p1_corrected=width*log_1_plus_exp_x((log_p1+squash_threshold)/width)-squash_threshold\n",
    "        log_p2_corrected=width*log_1_plus_exp_x((log_p2+squash_threshold)/width)-squash_threshold\n",
    "#         log_p1_corrected=width*torch.log(1+torch.exp((log_p1+squash_threshold)/width))-squash_threshold\n",
    "#         log_p2_corrected=width*torch.log(1+torch.exp((log_p2+squash_threshold)/width))-squash_threshold\n",
    "        s1a_b = log_p1_corrected - log_p2_corrected\n",
    "        log_p1=log_p(-s1a_b/gamma)\n",
    "        log_p2=log_1_minus_p(-s1a_b/gamma)\n",
    "        choice_NLL=-torch.stack([log_p1,log_p2],dim=-1)\n",
    "#         p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "#         p2=1-p1\n",
    "#         choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))\n",
    "        return choice_NLL\n",
    "\n",
    "\n",
    "class VariableWidthSquashingVariableGamma(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['squashes']=torch.ones(self.n_models,device=self.device)*200.0 # this doesn't work\n",
    "        parameters['gamma']=torch.ones(self.n_models,device=self.device)*10.0\n",
    "        parameters['width']=torch.ones(self.n_models,device=self.device)\n",
    "\n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        squash_threshold=cur_parameters['squashes']     \n",
    "        width=torch.nn.Softplus()(cur_parameters['width'])+1e-1\n",
    "        log_p1_corrected=width*log_1_plus_exp_x((log_p1+squash_threshold)/width)-squash_threshold\n",
    "        log_p2_corrected=width*log_1_plus_exp_x((log_p2+squash_threshold)/width)-squash_threshold\n",
    "#         log_p1_corrected=width*torch.log(1+math.e**((log_p1+squash_threshold)/width))-squash_threshold\n",
    "#         log_p2_corrected=width*torch.log(1+math.e**((log_p2+squash_threshold)/width))-squash_threshold\n",
    "        s1a_b = log_p1_corrected - log_p2_corrected\n",
    "        log_p1=log_p(-s1a_b/gamma)\n",
    "        log_p2=log_1_minus_p(-s1a_b/gamma)\n",
    "        choice_NLL=-torch.stack([log_p1,log_p2],dim=-1)\n",
    "#         p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "#         p2=1-p1\n",
    "#         choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))\n",
    "        return choice_NLL\n",
    "    \n",
    "\n",
    "class VariableGammaNoSquash(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['gamma']=torch.ones(self.n_models,device=self.device)*10.0\n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        s1a_b = log_p1 - log_p2\n",
    "        p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "        p2=1-p1\n",
    "        choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))\n",
    "        return choice_NLL\n",
    "    \n",
    "class SquashedSoftmax(ModelToHumanDecision):\n",
    "    def _initial_parameters(self):\n",
    "        parameters={}\n",
    "        parameters['gamma']=torch.ones(self.n_models,device=self.device)*10.0\n",
    "        parameters['eta']=torch.ones(self.n_models,device=self.device)*(0.0)\n",
    "        return parameters\n",
    "    def _f(self,log_p1,log_p2,cur_parameters):\n",
    "        gamma=cur_parameters['gamma']\n",
    "        eta=cur_parameters['eta']\n",
    "        \n",
    "        denominator=torch.logsumexp(torch.stack([log_p1/gamma,torch.ones_like(log_p1)*eta,log_p2/gamma,torch.ones_like(log_p2)*eta],dim=-1),dim=-1)\n",
    "        log_p1 = torch.logsumexp(torch.stack([log_p1/gamma,torch.ones_like(log_p1)*eta],dim=-1),dim=-1)-denominator\n",
    "        log_p2 = torch.logsumexp(torch.stack([log_p2/gamma,torch.ones_like(log_p2)*eta],dim=-1),dim=-1)-denominator\n",
    "        choice_NLL=-torch.stack([log_p1,log_p2],dim=-1)\n",
    "        return choice_NLL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "loss: tensor(1334.3275, grad_fn=<AddBackward0>)\n",
      "squashes tensor([166.9858, 153.9043, 138.1582, 139.9895, 119.7223, 124.2057, 122.7220,\n",
      "        130.9030, 119.5330, 111.7632, 113.3122], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "gamma tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "width tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "\n",
      "loss: tensor(830.4211, grad_fn=<AddBackward0>)\n",
      "squashes tensor([129.9776, 119.2324, 100.9770, 103.3657, 116.3093, 119.7868,  84.8777,\n",
      "        121.1108,  92.6890,  63.7944,  76.3746], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "gamma tensor([29.5141, 30.9084, 29.6017, 28.4889, 25.5749, 26.8265, 29.5939, 26.3070,\n",
      "        25.7912, 32.3243, 22.1221], dtype=torch.float64, requires_grad=True)\n",
      "width tensor([26.1388, 32.5998, 33.2086, 24.7605, 28.0871, 27.9913, 22.7964, 17.7990,\n",
      "        17.0634, 25.5470, -0.3360], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "loss: tensor(826.9465, grad_fn=<AddBackward0>)\n",
      "squashes tensor([127.8852, 118.7856,  97.1281,  88.3866, 125.5903, 132.3411,  81.7686,\n",
      "        122.2708,  82.6681,  60.3197,  74.4571], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "gamma tensor([29.1361, 31.9829, 28.1866, 24.5749, 25.7808, 28.2333, 28.3124, 28.6292,\n",
      "        23.2537, 31.7595, 18.0004], dtype=torch.float64, requires_grad=True)\n",
      "width tensor([20.7989, 38.6341, 30.7515, 11.6736, 35.5406, 32.4183,  4.8467, 10.4495,\n",
      "         5.3360,  3.8395, -5.2492], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "{'squashes': array([127.76018361, 119.04915886,  95.82144558,  85.80683343,\n",
      "       127.33750102, 134.95986818,  83.62531798, 120.41803792,\n",
      "        81.89917863,  61.60729273,  74.46242755]), 'gamma': array([28.95605297, 32.15228599, 27.81833135, 23.21243803, 25.80212136,\n",
      "       28.52640595, 28.42243485, 29.2312504 , 22.78896008, 31.45187176,\n",
      "       17.98020738]), 'width': array([ 1.91218679e+01,  3.98784626e+01,  3.01269543e+01,  1.08554438e+01,\n",
      "        3.68252940e+01,  3.29005280e+01, -1.69547166e-02,  6.91114380e+00,\n",
      "        3.71370926e+00,  3.84435901e-01, -5.32371492e+00])}\n",
      "total loss: tensor(826.4185, grad_fn=<AddBackward0>)\n",
      "bigram 79.86773681640625\n",
      "trigram 79.71249389648438\n",
      "rnn 71.65823364257812\n",
      "lstm 73.37562561035156\n",
      "bilstm 74.66841125488281\n",
      "bert 74.65912628173828\n",
      "bert_whole_word 79.67436981201172\n",
      "roberta 71.05909729003906\n",
      "xlm 70.9908676147461\n",
      "electra 84.3250732421875\n",
      "gpt2 66.42748260498047\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "gamma_boots=[]\n",
    "\n",
    "# parameters={'squashes':minps,'gamma':10.0}\n",
    "# model_class=FixedWidthSquashing\n",
    "\n",
    "# parameters={'squashes':minps,'gamma':10.0,'width':np.ones(len(models))}\n",
    "# model_class=VariableWidthSquashing\n",
    "\n",
    "# parameters={'squashes':minps,'gamma':np.ones_like(minps)*10.0}\n",
    "# model_class=FixedWidthSquashingVariableGamma\n",
    "\n",
    "# parameters={'gamma':np.ones_like(minps)*10.0}\n",
    "# # model_class=VariableGammaNoSquash\n",
    "\n",
    "parameters={'squashes':minps,'gamma':np.ones_like(minps)*10.0,'width':np.ones(len(models))}\n",
    "model_class=VariableWidthSquashingVariableGamma\n",
    "\n",
    "# parameters=None\n",
    "# model_class=SquashedSoftmax\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "for boot in range(1):\n",
    "    \n",
    "#    boots=np.random.choice(13, 13)\n",
    "    boots=np.arange(13)\n",
    "    \n",
    "    fitset=[]\n",
    "    for b in boots:\n",
    "        fitset+=fitsets[b]\n",
    "    \n",
    "    decision_model=model_class(model_name_list=models,parameters=parameters,device=device)\n",
    "\n",
    "    for par in decision_model.parameters.values():\n",
    "        par.requires_grad=True\n",
    "        \n",
    "    opt = optim.Adam(decision_model.parameters.values(),lr=1)\n",
    "\n",
    "    print(boot)\n",
    "    print('')\n",
    "\n",
    "    # build models by trials by sentences log-prob matrix\n",
    "    \n",
    "    log_p1=defaultdict(list)\n",
    "    log_p2=defaultdict(list)\n",
    "    response_ind=defaultdict(list)\n",
    "    for trial in fitset:\n",
    "        model1_name=trial[0]\n",
    "        log_p1[model1_name].append(trial[1])\n",
    "        log_p2[model1_name].append(trial[2])\n",
    "        response_ind[model1_name].append(trial[3])\n",
    "    \n",
    "    for epoch in range(200):\n",
    "\n",
    "        loss = torch.tensor(0.,device=device,requires_grad = True)\n",
    "        \n",
    "        model_loss=torch.zeros((len(models)),device=device)\n",
    "        for i_model,model1_name in enumerate(models):\n",
    "            log_p_sent=decision_model.decision_NLL(log_p1[model1_name],log_p2[model1_name],model=model1_name)\n",
    "            take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "            log_p_choice=take_by_2nd_dim(log_p_sent,response_ind[model1_name])\n",
    "            model_loss[i_model]=log_p_choice.sum()\n",
    "            loss=loss+model_loss[i_model]\n",
    "            \n",
    "        if epoch%90==0:\n",
    "            print('loss:',loss)\n",
    "            for par_name, par in decision_model.parameters.items():\n",
    "                print(par_name,par)\n",
    "            print('')\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    \n",
    "    print(decision_model.get_parameters())\n",
    "    print('total loss:',loss)\n",
    "    for i_model,model1_name in enumerate(models):\n",
    "        print(model1_name,model_loss[i_model].item())\n",
    "    \n",
    "    squashes=decision_model.get_parameters()['squashes']\n",
    "    if boot==0:\n",
    "        squash_boots=squashes\n",
    "    else:\n",
    "        squash_boots=np.concatenate((squash_boots,squashes),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "tensor(1335.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([166.9858, 153.9043, 138.1582, 139.9895, 119.7223, 124.2057, 122.7220,\n",
      "        130.9030, 119.5330, 111.7632, 113.3122], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(10., device='cuda:0', requires_grad=True)\n",
      "\n",
      "tensor(830.0360, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([129.8057, 129.6920,  84.4794,  91.8119, 112.0151, 105.4614,  84.4088,\n",
      "        117.2499,  88.0573,  60.0628,  81.5594], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(30.3510, device='cuda:0', requires_grad=True)\n",
      "\n",
      "tensor(829.7328, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([129.3686, 128.2721,  83.7435,  91.7080, 111.6073, 104.7709,  83.5495,\n",
      "        115.9641,  86.2835,  60.2307,  80.3986], device='cuda:0',\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor(28.9770, device='cuda:0', requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma_boots=[]\n",
    "\n",
    "for boot in range(1):\n",
    "    \n",
    "#    boots=np.random.choice(13, 13)\n",
    "    boots=np.arange(13)\n",
    "    \n",
    "    fitset=[]\n",
    "    for b in boots:\n",
    "        fitset+=fitsets[b]\n",
    "    \n",
    "    squashes=Variable(torch.tensor(minps).to('cuda'), requires_grad = True)\n",
    "    gamma=Variable(torch.tensor(10.).to('cuda'), requires_grad = True)\n",
    "\n",
    "    opt = optim.Adam([squashes,gamma],lr=1)\n",
    "\n",
    "    print(boot)\n",
    "    print('')\n",
    "\n",
    "    for epoch in range(200):\n",
    "\n",
    "        loss = Variable(torch.tensor(0.).to('cuda'), requires_grad = True)   \n",
    "\n",
    "        for trial in fitset:\n",
    "\n",
    "            model1_name=trial[0]\n",
    "\n",
    "            log_p1=torch.tensor(trial[1]).to('cuda')\n",
    "            log_p2=torch.tensor(trial[2]).to('cuda')\n",
    "\n",
    "            response_ind=trial[3]\n",
    "\n",
    "            squash_threshold=squashes[models.index(model1_name)]\n",
    "\n",
    "            width=1\n",
    "\n",
    "            log_p1_corrected=width*torch.log(1+math.e**((log_p1+squash_threshold)/width))-squash_threshold\n",
    "            log_p2_corrected=width*torch.log(1+math.e**((log_p2+squash_threshold)/width))-squash_threshold\n",
    "\n",
    "            s1a_b = log_p1_corrected - log_p2_corrected\n",
    "\n",
    "            p1=1/(1+torch.exp(-(s1a_b)/gamma))\n",
    "            p2=1-p1\n",
    "\n",
    "            log_p_choice=-torch.log([p1,p2][response_ind])\n",
    "\n",
    "            loss=loss+log_p_choice\n",
    "\n",
    "\n",
    "        if epoch%90==0:\n",
    "            print(loss)\n",
    "            print(squashes)\n",
    "            print(gamma)\n",
    "            print('')\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    squashes=squashes.detach().cpu().numpy()\n",
    "    gamma=float(gamma)\n",
    "\n",
    "    gamma_boots.append(gamma)\n",
    "    \n",
    "    if boot==0:\n",
    "        squash_boots=squashes\n",
    "    else:\n",
    "        squash_boots=np.concatenate((squash_boots,squashes),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synthetic\n",
    "\n",
    "# tensor(671.4779, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.5626, 128.7844,  84.1193,  92.4041, 111.8108, 105.3416,  83.8154,\n",
    "#         116.5818,  87.0363,  60.6492,  81.0560], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([29.8585, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "#natural\n",
    "\n",
    "# tensor(148.7837, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([115.8415,  84.2844,  72.0876,  43.8699,  54.9998,  50.4821,  69.6983,\n",
    "#          49.1065,  42.4528,  34.6840,  59.5816], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([ 9.1418, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "# tensor(829.6835, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.1968, 127.7177,  83.4297,  91.0015, 111.4391, 104.3089,  83.3092,\n",
    "#         115.3282,  85.6379,  59.8025,  79.7266], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([28.2664, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots=np.array([[136.6031, 130.6315,  88.2857, 127.1014, 124.6718, 110.4878,  84.6417,\n",
    "        118.1435, 107.7737,  63.0790,  79.9143],\n",
    "              [137.9003, 125.7105,  84.5635, 127.1772, 112.0871,  94.5774,  78.6344,\n",
    "        114.0863,  95.4273,  58.2541,  79.6323],\n",
    "              [131.8761, 133.7181, 147.7142, 143.2530, 110.6293, 134.2203,  83.7578,\n",
    "        139.0220,  82.7407,  66.7261,  79.2919],\n",
    "              [139.6443, 135.6945,  80.5810,  94.8058, 116.6113, 103.0531,  87.4392,\n",
    "        119.7775,  81.2322,  60.9193,  81.1773],\n",
    "              [132.4967, 128.7608,  92.4549,  91.8489, 110.4346, 104.1544,  80.5317,\n",
    "        126.0785, 124.9601,  57.4880,  76.9285],\n",
    "              [128.2988, 112.9426,  80.0811,  90.9844, 111.0652, 115.8396,  56.3730,\n",
    "         78.9889, 120.2621,  29.6331,  61.9582],\n",
    "              [127.7181, 132.9688, 148.8201,  94.7272, 109.7225,  87.5404,  81.3132,\n",
    "        115.8013, 116.9241,  64.7583,  73.5550],\n",
    "              [120.8540, 116.8421, 135.4461,  88.5892, 124.1079, 133.7935,  86.1704,\n",
    "        100.1683,  92.7386,  49.7565,  79.4477],\n",
    "              [125.4391, 114.5180,  82.5459,  87.4416, 110.8198,  93.8739,  63.4568,\n",
    "        117.9814,  83.9064,  57.2624,  77.2107],\n",
    "              [129.6447, 129.3590,  86.6444,  92.5276, 109.5141, 105.0256,  65.8152,\n",
    "        117.5682, 125.8277,  59.4612,  77.8789]])\n",
    "\n",
    "\n",
    "squash_boots=squash_boots.T\n",
    "squash_boots.sort()\n",
    "squash_boots=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squash_boots=squash_boots.reshape([10,11])\n",
    "\n",
    "\n",
    "\n",
    "x=np.arange(11)*2\n",
    "plt.bar(x-0.2, maxps, width=0.1, color=[1,0,0])\n",
    "plt.bar(x-0.1, squash_boots[0], width=0.1, color=[0,0,1])\n",
    "plt.bar(x, squash_boots[1], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.1, squash_boots[2], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.2, squash_boots[3], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.3, squash_boots[4], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.4, squash_boots[5], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.5, squash_boots[6], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.6, squash_boots[7], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.7, squash_boots[8], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.8, squash_boots[9], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.9, minps, width=0.1, color=[1,0,0])\n",
    "\n",
    "plt.ylabel('Squash Thresholds')\n",
    "plt.xticks(x,models,rotation=70)\n",
    "\n",
    "legs=['Min/Max baseline','Squash thresholds']\n",
    "plt.legend(legs,bbox_to_anchor=(1, 0.3, 0.2, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fitset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=np.array([f.sort() for f in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots=d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
