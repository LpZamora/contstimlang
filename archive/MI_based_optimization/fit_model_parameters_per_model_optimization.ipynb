{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, pickle, time, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.dpi']= 200\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import model_to_human_decision_torch \n",
    "from model_to_human_decision_torch import get_parameters_across_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data_csv='data_exp1_cumulative.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/anaconda3/envs/contstimlang/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 70 within-subject repeating trials.\n"
     ]
    }
   ],
   "source": [
    "def prepare_tidy_protocol(csv_path,do_remove_within_subject_repetitions=True):\n",
    "    df=pd.read_csv(csv_path)\n",
    "    \n",
    "    df=df.loc[df.Response.notna(),:] #drop non-response lines\n",
    "    df=df.rename(columns={'source':'source_set 1'}) # fix a mistake in model naming\n",
    "    df=df.rename(columns={'counterbalance-o1ql':'set_num'})\n",
    "\n",
    "    # transform set columns to trial columns\n",
    "    p = re.compile('(.+)_set \\d+')\n",
    "    set_columns=[s for s in list(df.columns) if p.match(s)]\n",
    "    columns_to_keep=['Participant External Session ID','Event Index','Reaction Time','Response','set_num']+set_columns\n",
    "    columns_to_drop=list(np.setdiff1d(df.columns,columns_to_keep))\n",
    "    df=df.drop(columns=columns_to_drop)\n",
    "    columns_to_build=list(np.unique([p.findall(s)[0] for s in set_columns]))\n",
    "    for index,row in df.iterrows():\n",
    "        cur_set=row['set_num']\n",
    "        for c in columns_to_build:\n",
    "            df.loc[index,c]=df.loc[index,c+'_'+cur_set]\n",
    "    df=df.drop(columns=set_columns+['set_num'])\n",
    "\n",
    "    # make sure the sentence pairs are alphabetically sorted\n",
    "    def sort_sentence_pairs(df1):    \n",
    "        flip_dict={'sentence1':'sentence2','sentence2':'sentence1',\n",
    "                   'sentence1_model':'sentence2_model','sentence2_model':'sentence1_model'}\n",
    "        df2=df1.copy()    \n",
    "        for index, row in df2.iterrows():\n",
    "            if row['sentence1']>row['sentence2']: # a flip is needed\n",
    "                for old_col, new_col in flip_dict.items():\n",
    "                    df2.loc[index,new_col]=df1.loc[index,old_col]\n",
    "        return df2\n",
    "    df=sort_sentence_pairs(df)\n",
    "\n",
    "    # separate models and levels\n",
    "    p=re.compile('(.+)_(\\d+)')\n",
    "    df['sentence1_model_name']=[p.findall(s)[0][0] for s in df['sentence1_model']]\n",
    "    df['sentence2_model_name']=[p.findall(s)[0][0] for s in df['sentence2_model']]\n",
    "    df['sentence1_model_level']=[int(p.findall(s)[0][1]) for s in df['sentence1_model']]\n",
    "    df['sentence2_model_level']=[int(p.findall(s)[0][1]) for s in df['sentence2_model']]\n",
    "    df=df.drop(columns=['sentence1_model','sentence2_model'])\n",
    "\n",
    "    # mark choice indecis\n",
    "    for index,row in df.iterrows():\n",
    "        df.loc[index,'Choice']=[row.sentence1, row.sentence2].index(row.Response)\n",
    "\n",
    "    df['sentence_pair_id']=[s1+'_'+s2 for s1,s2 in zip(df.sentence1, df.sentence2)]\n",
    "\n",
    "    # renumber subjects\n",
    "    df.drop(columns='subject') # there's some issue here    \n",
    "    uq_subjects, ind, unique_inverse = np.unique(df['Participant External Session ID'], return_index=True,return_inverse=True)\n",
    "    uq_subjects=list(uq_subjects[np.argsort(ind)]) # unique subjects by order of appearance    \n",
    "    df['subject']=[uq_subjects.index(s) for s in df['Participant External Session ID']]\n",
    "                   \n",
    "    # optionally, remove all but first occurance of each sentence pair within a subject    \n",
    "    if do_remove_within_subject_repetitions:\n",
    "        def remove_within_subject_repetitions(df1):\n",
    "            # keep only the first appearance of each sentence pair within a subject\n",
    "            _,unique_indices=np.unique(df1.sentence_pair_id,return_index=True)\n",
    "            df2=df1.loc[np.in1d(np.arange(len(df1)),unique_indices),:]    \n",
    "            return df2\n",
    "        old_length=len(df)\n",
    "        df=df.groupby('subject').apply(remove_within_subject_repetitions)\n",
    "        print(\"removed {} within-subject repeating trials.\".format(old_length-len(df)))\n",
    "        \n",
    "    df=df.set_index(['subject','trial']).sort_values(by=['subject','trial'],axis=0).reset_index()\n",
    "    \n",
    "    # a final sanity check: \n",
    "    assert len(np.unique(df['sentence_pair_id']))==len(df.drop_duplicates(subset=['sentence1','sentence2'])), 'number of sentences pairs must match number of sentence pair ids'\n",
    "    \n",
    "    return df\n",
    "protocol=prepare_tidy_protocol(task_data_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2570 total trials with 605 sentence pairs and 24 subjects.\n"
     ]
    }
   ],
   "source": [
    "n_total_trials=len(protocol)\n",
    "n_sentence_pairs=len(np.unique(protocol['sentence_pair_id']))\n",
    "n_subjects=len(np.unique(protocol['Participant External Session ID']))\n",
    "print(\"loaded {} total trials with {} sentence pairs and {} subjects.\".format(n_total_trials,n_sentence_pairs,n_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>Event Index</th>\n",
       "      <th>Participant External Session ID</th>\n",
       "      <th>Reaction Time</th>\n",
       "      <th>Response</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence1_model_name</th>\n",
       "      <th>sentence2_model_name</th>\n",
       "      <th>sentence1_model_level</th>\n",
       "      <th>sentence2_model_level</th>\n",
       "      <th>Choice</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>5812.220000</td>\n",
       "      <td>What public schools teach their students is co...</td>\n",
       "      <td>I can not long conceal my dreadful sin</td>\n",
       "      <td>What public schools teach their students is co...</td>\n",
       "      <td>internet</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I can not long conceal my dreadful sin_What pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>5300.000000</td>\n",
       "      <td>And that consists of a number of factors</td>\n",
       "      <td>And that consists of a number of factors</td>\n",
       "      <td>Birmingham but the heavy duty cell one and</td>\n",
       "      <td>generator</td>\n",
       "      <td>electra</td>\n",
       "      <td>electra</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And that consists of a number of factors_Birmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>9964.815000</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubts</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubts</td>\n",
       "      <td>Mel causal that foothold overflowed paid Cpl a...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>11878.520000</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "      <td>The frustrating and bananas and the modificati...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bigram</td>\n",
       "      <td>bigram</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>12686.870000</td>\n",
       "      <td>Listening those studied bails heater understan...</td>\n",
       "      <td>Hours into cults cables help arrive in mounts</td>\n",
       "      <td>Listening those studied bails heater understan...</td>\n",
       "      <td>generator</td>\n",
       "      <td>lstm</td>\n",
       "      <td>lstm</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hours into cults cables help arrive in mounts_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>214</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>1467.500000</td>\n",
       "      <td>List of people in America who hate it</td>\n",
       "      <td>Blog negative blog shelve grammar represents c...</td>\n",
       "      <td>List of people in America who hate it</td>\n",
       "      <td>generator</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blog negative blog shelve grammar represents c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>23</td>\n",
       "      <td>107.0</td>\n",
       "      <td>216</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2621.625001</td>\n",
       "      <td>Lions in his most charming little extraterrest...</td>\n",
       "      <td>Forces bucks hometown donors Berger complicati...</td>\n",
       "      <td>Lions in his most charming little extraterrest...</td>\n",
       "      <td>generator</td>\n",
       "      <td>trigram</td>\n",
       "      <td>trigram</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forces bucks hometown donors Berger complicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>23</td>\n",
       "      <td>108.0</td>\n",
       "      <td>218</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2137.399999</td>\n",
       "      <td>Immediately after completing algorithms the ga...</td>\n",
       "      <td>Danny Louvre shrouded typically flees offendin...</td>\n",
       "      <td>Immediately after completing algorithms the ga...</td>\n",
       "      <td>generator</td>\n",
       "      <td>roberta</td>\n",
       "      <td>roberta</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Danny Louvre shrouded typically flees offendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>23</td>\n",
       "      <td>109.0</td>\n",
       "      <td>220</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>1881.740000</td>\n",
       "      <td>The man sleeps with the second solid object</td>\n",
       "      <td>Hart bitter motions Toledo going whacking emer...</td>\n",
       "      <td>The man sleeps with the second solid object</td>\n",
       "      <td>generator</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hart bitter motions Toledo going whacking emer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>23</td>\n",
       "      <td>110.0</td>\n",
       "      <td>222</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2625.890001</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "      <td>Want sits preaching subscription as spotlights...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bigram</td>\n",
       "      <td>bigram</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject  trial Event Index Participant External Session ID  \\\n",
       "0           0    1.0           4        5f89a28900ebf4068239ad58   \n",
       "1           0    2.0           6        5f89a28900ebf4068239ad58   \n",
       "2           0    3.0           8        5f89a28900ebf4068239ad58   \n",
       "3           0    4.0          10        5f89a28900ebf4068239ad58   \n",
       "4           0    5.0          12        5f89a28900ebf4068239ad58   \n",
       "...       ...    ...         ...                             ...   \n",
       "2565       23  106.0         214        5fde554b85dbbb5cf9a1d168   \n",
       "2566       23  107.0         216        5fde554b85dbbb5cf9a1d168   \n",
       "2567       23  108.0         218        5fde554b85dbbb5cf9a1d168   \n",
       "2568       23  109.0         220        5fde554b85dbbb5cf9a1d168   \n",
       "2569       23  110.0         222        5fde554b85dbbb5cf9a1d168   \n",
       "\n",
       "      Reaction Time                                           Response  \\\n",
       "0       5812.220000  What public schools teach their students is co...   \n",
       "1       5300.000000           And that consists of a number of factors   \n",
       "2       9964.815000    Dad of a stagnant fermentation shrimp of doubts   \n",
       "3      11878.520000  Brisbane corp distant and finished commandos c...   \n",
       "4      12686.870000  Listening those studied bails heater understan...   \n",
       "...             ...                                                ...   \n",
       "2565    1467.500000              List of people in America who hate it   \n",
       "2566    2621.625001  Lions in his most charming little extraterrest...   \n",
       "2567    2137.399999  Immediately after completing algorithms the ga...   \n",
       "2568    1881.740000        The man sleeps with the second solid object   \n",
       "2569    2625.890001  Anyone electron elevation ponies circulate the...   \n",
       "\n",
       "                                              sentence1  \\\n",
       "0                I can not long conceal my dreadful sin   \n",
       "1              And that consists of a number of factors   \n",
       "2       Dad of a stagnant fermentation shrimp of doubts   \n",
       "3     Brisbane corp distant and finished commandos c...   \n",
       "4         Hours into cults cables help arrive in mounts   \n",
       "...                                                 ...   \n",
       "2565  Blog negative blog shelve grammar represents c...   \n",
       "2566  Forces bucks hometown donors Berger complicati...   \n",
       "2567  Danny Louvre shrouded typically flees offendin...   \n",
       "2568  Hart bitter motions Toledo going whacking emer...   \n",
       "2569  Anyone electron elevation ponies circulate the...   \n",
       "\n",
       "                                              sentence2     source  \\\n",
       "0     What public schools teach their students is co...   internet   \n",
       "1            Birmingham but the heavy duty cell one and  generator   \n",
       "2     Mel causal that foothold overflowed paid Cpl a...  generator   \n",
       "3     The frustrating and bananas and the modificati...  generator   \n",
       "4     Listening those studied bails heater understan...  generator   \n",
       "...                                                 ...        ...   \n",
       "2565              List of people in America who hate it  generator   \n",
       "2566  Lions in his most charming little extraterrest...  generator   \n",
       "2567  Immediately after completing algorithms the ga...  generator   \n",
       "2568        The man sleeps with the second solid object  generator   \n",
       "2569  Want sits preaching subscription as spotlights...  generator   \n",
       "\n",
       "     sentence1_model_name sentence2_model_name  sentence1_model_level  \\\n",
       "0                     xlm                  xlm                      1   \n",
       "1                 electra              electra                      9   \n",
       "2                  bilstm               bilstm                      5   \n",
       "3                  bigram               bigram                      2   \n",
       "4                    lstm                 lstm                      5   \n",
       "...                   ...                  ...                    ...   \n",
       "2565                  xlm                  xlm                      3   \n",
       "2566              trigram              trigram                      1   \n",
       "2567              roberta              roberta                      1   \n",
       "2568                  xlm                  xlm                      1   \n",
       "2569               bigram               bigram                      1   \n",
       "\n",
       "      sentence2_model_level  Choice  \\\n",
       "0                         0     1.0   \n",
       "1                         5     0.0   \n",
       "2                         0     0.0   \n",
       "3                         6     0.0   \n",
       "4                         2     1.0   \n",
       "...                     ...     ...   \n",
       "2565                      9     1.0   \n",
       "2566                      5     1.0   \n",
       "2567                      6     1.0   \n",
       "2568                      8     1.0   \n",
       "2569                      2     0.0   \n",
       "\n",
       "                                       sentence_pair_id  \n",
       "0     I can not long conceal my dreadful sin_What pu...  \n",
       "1     And that consists of a number of factors_Birmi...  \n",
       "2     Dad of a stagnant fermentation shrimp of doubt...  \n",
       "3     Brisbane corp distant and finished commandos c...  \n",
       "4     Hours into cults cables help arrive in mounts_...  \n",
       "...                                                 ...  \n",
       "2565  Blog negative blog shelve grammar represents c...  \n",
       "2566  Forces bucks hometown donors Berger complicati...  \n",
       "2567  Danny Louvre shrouded typically flees offendin...  \n",
       "2568  Hart bitter motions Toledo going whacking emer...  \n",
       "2569  Anyone electron elevation ponies circulate the...  \n",
       "\n",
       "[2570 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate negative log-likelihood of choice predictions given as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NLL(subject_data,predictions,mode='sum',deal_with_missing_predictions='omit',return_non_agg_NLL=False):\n",
    "    \"\"\" evaluate the negative-log-likelihood of human response data given model predictions.\n",
    "    \n",
    "    subject_data (pd.DataFrame) a dataframe such as the one returned from prepare_tidy_protocol. Should contain 'subject' (str), 'sentence_pair_id' (str) and 'Choice' (either 0 or 1).\n",
    "    predictions (pd.DataFrame) model predictions. \n",
    "        Should contain sentence_pair_id (str), and either 'choice_0_prob' and 'choice_1_prob' or 'choice_0_log_prob' and 'choice_1_log_prob': \n",
    "        the (log) probabilitiy of choosing sentence 0 or the other one.\n",
    "        the predictions dataframe can contain also a 'subject' column for subject specific-predictions\n",
    "    mode (str) 'sum' or 'mean' (how NLL is aggregeated).\n",
    "    deal_with_missing_predictions (str) 'omit' - deletes trials with no predictions. uniform predictions - uses a p=0.5 prediction.\n",
    "    return_non_agg_NLL (boolean)\n",
    "    \n",
    "    returns:\n",
    "    if return_non_agg_NLL is True, (aggregated dataframe, original dataframe)\n",
    "    else, only the aggregated dataframe\n",
    "    both return dataframes contain a new columns NLL - negative-log-likelihood\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'subject' in predictions.columns: # predictions are subject-specific\n",
    "        df=subject_data.merge(predictions,how='left',on=['sentence_pair_id','subject'])\n",
    "    else: # same predictions for all subjects\n",
    "        df=subject_data.merge(predictions,how='left',on=['sentence_pair_id'])\n",
    "    c=np.asarray(subject_data.Choice,dtype=float)\n",
    "    \n",
    "    if 'choice_0_log_prob' in predictions.columns and 'choice_1_log_prob' in predictions.columns:\n",
    "        log_p=np.where(c==0,df.choice_0_log_prob,df.choice_1_log_prob)\n",
    "        log_p=np.where(np.isnan(c),np.nan,log_p)\n",
    "    elif 'choice_0_prob' in predictions.columns and 'choice_1_prob' in predictions.columns:\n",
    "        p=np.where(c==0,df.choice_0_prob,df.choice_1_prob)\n",
    "        p=np.where(np.isnan(c),np.nan,p)\n",
    "        log_p=np.log(p)\n",
    "    else:\n",
    "        raise ValueError('predictions must include either choice_0_log_prob and choice_1_log_prob or choice_0_prob and choice_1_prob')\n",
    "    \n",
    "    df['NLL']=-log_p\n",
    "    \n",
    "    # deal with trials for which we have no predictions:\n",
    "    missing_predictions=np.logical_and(df['NLL'].isna(),np.logical_not(np.isnan(c))) # there's no prediction but the subject made a choice\n",
    "    \n",
    "    if np.any(missing_predictions):\n",
    "        print('Found {} missing predictions. Handling strategy: {}.'.format(np.sum(missing_predictions),deal_with_missing_predictions))\n",
    "        if deal_with_missing_predictions=='omit':\n",
    "            df=df.loc[np.logical_not(missing_predictions),:] # drop these trails\n",
    "        elif deal_with_missing_predictions=='uniform predictions':\n",
    "            df[missing_predictions,'NLL']=-np.log(0.5) # make a uniform prediction\n",
    "        else:\n",
    "            raise ValueError(\"invalid deal_with_missing_predictions\")\n",
    "    \n",
    "    # aggregate NLL\n",
    "    if mode=='sum':\n",
    "        agg_NLL=df.NLL.sum().item()        \n",
    "    elif mode=='mean':\n",
    "        agg_NLL=df.NLL.mean().item()\n",
    "    if return_non_agg_NLL:\n",
    "        return agg_NLL, df\n",
    "    else:\n",
    "        return agg_NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper and lower bounds on the noise ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower noise ceiling NLL (before fitting) 1685.7361062606578\n",
      "upper noise ceiling NLL 1142.902361691926\n"
     ]
    }
   ],
   "source": [
    "def get_ub_noise_ceiling_predictions(subject_data):    \n",
    "    # calculate human choice probability per sentence (collapsed over subjects and within-subject repetitions)\n",
    "    df=subject_data.copy()\n",
    "    df['chose_0']=np.asarray(subject_data.Choice==0,np.float)\n",
    "    df['chose_0'].loc[np.isnan(subject_data.Choice)]==np.nan\n",
    "    df2=df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].mean()\n",
    "    df2=df2.drop(columns=['subject']).reset_index().rename(columns={'chose_0':'choice_0_prob'})\n",
    "    df2['choice_1_prob']=1.0-np.asarray(df2.choice_0_prob)\n",
    "    return df2\n",
    "ub_NC_predictions=get_ub_noise_ceiling_predictions(protocol)\n",
    "\n",
    "def get_lb_noise_ceiling_predictions(subject_data,pseudo_counts=1.0):\n",
    "    # calculate leave-one-subject out human choice probability per sentence (collapsed over subjects and within-subject repetitions)\n",
    "    \n",
    "    # pseudo_counts controls the symmetric beta prior (alpha=beta=pseudo_counts+1)\n",
    "    subjects=list(np.unique(subject_data.subject))\n",
    "    df1=subject_data.copy()\n",
    "    df1['chose_0']=np.asarray(df1.Choice==0,np.float)\n",
    "    df1['chose_0'].loc[np.isnan(df1.Choice)]==np.nan\n",
    "    \n",
    "    subject_specific_predictions_dfs=[]\n",
    "    for subject in subjects: # leave one-subject out cross-validation:\n",
    "        without_subject_df=df1.copy().loc[df1.subject!=subject,:] # hold out one subject\n",
    "        if pseudo_counts!=0: # Bayesian prior\n",
    "            choice_0_counts_without_subject=without_subject_df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].sum()\n",
    "            total_counts_without_subject=without_subject_df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].count()\n",
    "            subject_specific_predictions=(choice_0_counts_without_subject+pseudo_counts)/(total_counts_without_subject+pseudo_counts*2)\n",
    "        else: # plain mean\n",
    "            subject_specific_predictions=without_subject_df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].mean() # mean choice probabilities of the remaining subjects                                    \n",
    "        subject_specific_predictions=subject_specific_predictions.reset_index().rename(columns={'chose_0':'choice_0_prob'})\n",
    "        subject_specific_predictions['choice_1_prob']=1.0-np.asarray(subject_specific_predictions.choice_0_prob)                \n",
    "        subject_specific_predictions.loc[:,'subject']=subject # we mark the resulting predictions as subject-specific predictions\n",
    "        \n",
    "        # next, we remove predictions for sentence pairs the subject didn't see:\n",
    "        cur_subject_df=df1.copy().loc[df1.subject==subject,:] \n",
    "        cur_subject_sentence_pair_ids=np.unique(cur_subject_df.sentence_pair_id) # these are the sentences the subject saw\n",
    "        subject_specific_predictions=subject_specific_predictions.loc[np.in1d(subject_specific_predictions.sentence_pair_id,cur_subject_sentence_pair_ids),:]                \n",
    "        subject_specific_predictions_dfs.append(subject_specific_predictions) # and add the remaining predictions to the list\n",
    "    \n",
    "    lb_NC_predictions=pd.concat(subject_specific_predictions_dfs)\n",
    "    \n",
    "    return lb_NC_predictions\n",
    "lb_NC_predictions=get_lb_noise_ceiling_predictions(protocol)\n",
    "lb_NC_NLL=evaluate_NLL(protocol,lb_NC_predictions,mode='sum')\n",
    "print(\"lower noise ceiling NLL (before fitting)\",lb_NC_NLL)\n",
    "\n",
    "\n",
    "ub_NC_NLL=evaluate_NLL(protocol,ub_NC_predictions,mode='sum')\n",
    "print(\"upper noise ceiling NLL\",ub_NC_NLL)\n",
    "\n",
    "# # without a Gamma parameter, the lower noise ceiling can be infinity\n",
    "# lb_NC_NLL,df=evaluate_NLL(protocol,lb_NC_predictions,mode='sum',return_non_agg_NLL=True)\n",
    "# print(\"lower noise ceiling NLL\",lb_NC_NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "      <th>choice_0_prob</th>\n",
       "      <th>choice_1_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A agony it Rapids and the bikini dipping</td>\n",
       "      <td>Most are the names of arrogance native land</td>\n",
       "      <td>A agony it Rapids and the bikini dipping_Most ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A agony it Rapids and the bikini dipping</td>\n",
       "      <td>The Marina is a sailing boat from Scotland</td>\n",
       "      <td>A agony it Rapids and the bikini dipping_The M...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A and had to the past it again</td>\n",
       "      <td>The overpowering the lawn of such a year</td>\n",
       "      <td>A and had to the past it again_The overpowerin...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A case of driver replacements involved John Bowe</td>\n",
       "      <td>It includes notable people in the modern era</td>\n",
       "      <td>A case of driver replacements involved John Bo...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A civilization that was the largest weighing in</td>\n",
       "      <td>Talent beaches mirroring the publicly block tr...</td>\n",
       "      <td>A civilization that was the largest weighing i...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>This tarnished devising of seconded cruelty an...</td>\n",
       "      <td>To retroactively your team of style and workout</td>\n",
       "      <td>This tarnished devising of seconded cruelty an...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Too much separates relax fabric choral rudder ...</td>\n",
       "      <td>Use it for this and many other things</td>\n",
       "      <td>Too much separates relax fabric choral rudder ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Trust reflect slugging draped robots shoe firs...</td>\n",
       "      <td>Worth a turkey license issuance fortune choppe...</td>\n",
       "      <td>Trust reflect slugging draped robots shoe firs...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Was excited when I saw the awesome formatting</td>\n",
       "      <td>We were fools not to have known it</td>\n",
       "      <td>Was excited when I saw the awesome formatting_...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>We need to have some real banking reform</td>\n",
       "      <td>What you say may very well be true</td>\n",
       "      <td>We need to have some real banking reform_What ...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "0             A agony it Rapids and the bikini dipping   \n",
       "1             A agony it Rapids and the bikini dipping   \n",
       "2                       A and had to the past it again   \n",
       "3     A case of driver replacements involved John Bowe   \n",
       "4      A civilization that was the largest weighing in   \n",
       "..                                                 ...   \n",
       "600  This tarnished devising of seconded cruelty an...   \n",
       "601  Too much separates relax fabric choral rudder ...   \n",
       "602  Trust reflect slugging draped robots shoe firs...   \n",
       "603      Was excited when I saw the awesome formatting   \n",
       "604           We need to have some real banking reform   \n",
       "\n",
       "                                             sentence2  \\\n",
       "0          Most are the names of arrogance native land   \n",
       "1           The Marina is a sailing boat from Scotland   \n",
       "2             The overpowering the lawn of such a year   \n",
       "3         It includes notable people in the modern era   \n",
       "4    Talent beaches mirroring the publicly block tr...   \n",
       "..                                                 ...   \n",
       "600    To retroactively your team of style and workout   \n",
       "601              Use it for this and many other things   \n",
       "602  Worth a turkey license issuance fortune choppe...   \n",
       "603                 We were fools not to have known it   \n",
       "604                 What you say may very well be true   \n",
       "\n",
       "                                      sentence_pair_id  choice_0_prob  \\\n",
       "0    A agony it Rapids and the bikini dipping_Most ...           0.00   \n",
       "1    A agony it Rapids and the bikini dipping_The M...           0.00   \n",
       "2    A and had to the past it again_The overpowerin...           0.25   \n",
       "3    A case of driver replacements involved John Bo...           0.25   \n",
       "4    A civilization that was the largest weighing i...           0.75   \n",
       "..                                                 ...            ...   \n",
       "600  This tarnished devising of seconded cruelty an...           0.25   \n",
       "601  Too much separates relax fabric choral rudder ...           0.00   \n",
       "602  Trust reflect slugging draped robots shoe firs...           0.75   \n",
       "603  Was excited when I saw the awesome formatting_...           0.25   \n",
       "604  We need to have some real banking reform_What ...           0.20   \n",
       "\n",
       "     choice_1_prob  \n",
       "0             1.00  \n",
       "1             1.00  \n",
       "2             0.75  \n",
       "3             0.75  \n",
       "4             0.25  \n",
       "..             ...  \n",
       "600           0.75  \n",
       "601           1.00  \n",
       "602           0.25  \n",
       "603           0.75  \n",
       "604           0.80  \n",
       "\n",
       "[605 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub_NC_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo sentence log-probabilities\n",
    "In order to deal with the noise ceiling estimates in the same fashion we deal with the deep nets' outputs, we convert their choice probabilities pseudo-sentence probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_sentence_probs(p_choice_1,p_choice_2,pseudo_sentence_prob_gamma=1.234,eps=1e-8):\n",
    "    p_choice_1=np.clip(p_choice_1,eps,1-eps)\n",
    "    p_choice_2=np.clip(p_choice_2,eps,1-eps)\n",
    "    \n",
    "    s1a_b=-np.log((1-p_choice_1)/p_choice_1)*pseudo_sentence_prob_gamma  \n",
    "    lp1=-100+s1a_b/2\n",
    "    lp2=-100-s1a_b/2\n",
    "\n",
    "    return lp1,lp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['bigram','trigram','rnn','lstm','bilstm','bert','bert_whole_word','roberta','xlm','electra','gpt2','ub_NC','lb_NC']\n",
    "\n",
    "comp_models=list(np.setdiff1d(models,['ub_NC','lb_NC']))\n",
    "human_models=list(np.setdiff1d(models,comp_models))\n",
    "\n",
    "def get_cached_sentence_log_probabilities_for_model(model_name,protocol):\n",
    "    \"\"\" build a pandas dataframe of model raw sentence probabilities using cached model outputs\"\"\"\n",
    "    f = open(model_name+'_expt1_sentence_probs.pkl','rb')\n",
    "    prob_dict=pickle.load(f)\n",
    "    model_predictions=protocol[['sentence1','sentence2','sentence_pair_id']].drop_duplicates().reset_index(drop=True) \n",
    "    for i_row, row in model_predictions.iterrows():\n",
    "        model_predictions.at[i_row,'sentence1_raw_log_prob']=prob_dict[row['sentence1']]\n",
    "        model_predictions.at[i_row,'sentence2_raw_log_prob']=prob_dict[row['sentence2']]\n",
    "    return model_predictions\n",
    "\n",
    "def noise_ceiling_to_pseudo_log_probabilities(NC_predictions):\n",
    "    \"\"\" recast trial-specific noise ceiling predictions to be language-model like \"\"\"\n",
    "    NC_predictions=NC_predictions.copy()\n",
    "    p1=NC_predictions['choice_0_prob']\n",
    "    p2=NC_predictions['choice_1_prob']\n",
    "    assert np.isfinite(p1).all() and np.isfinite(p2).all(), 'found nan predictions for subject {}: {}).'.format(trial.subject,trial.sentence_pair_id)\n",
    "    log_p1,log_p2=pseudo_sentence_probs(p1,p2)\n",
    "    NC_predictions['sentence1_raw_log_prob']=log_p1\n",
    "    NC_predictions['sentence2_raw_log_prob']=log_p2\n",
    "    NC_predictions.drop(columns=['choice_0_prob','choice_1_prob'])\n",
    "    return NC_predictions\n",
    "\n",
    "model_sentence_predictions=OrderedDict() # this will hold a dataframe for each model\n",
    "\n",
    "for model_name in models:\n",
    "    if model_name in comp_models:\n",
    "        model_sentence_predictions[model_name]=get_cached_sentence_log_probabilities_for_model(model_name,protocol)\n",
    "    elif model_name=='ub_NC':\n",
    "        model_sentence_predictions[model_name]=noise_ceiling_to_pseudo_log_probabilities(ub_NC_predictions)\n",
    "    elif model_name=='lb_NC':\n",
    "        model_sentence_predictions[model_name]=noise_ceiling_to_pseudo_log_probabilities(lb_NC_predictions)\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "      <th>choice_0_prob</th>\n",
       "      <th>choice_1_prob</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentence1_raw_log_prob</th>\n",
       "      <th>sentence2_raw_log_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A naturally occurring replies punishment exami...</td>\n",
       "      <td>This country uses collaborative ping internal ...</td>\n",
       "      <td>A naturally occurring replies punishment exami...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>-100.993023</td>\n",
       "      <td>-99.006977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A poets piped enquiry a lot of it</td>\n",
       "      <td>The growing spectral and thus attain the bandit</td>\n",
       "      <td>A poets piped enquiry a lot of it_The growing ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A poster and employer approval rating and vows</td>\n",
       "      <td>The next to the time and historical park</td>\n",
       "      <td>A poster and employer approval rating and vows...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.572328</td>\n",
       "      <td>-100.427672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Also their navy is in a dismal state</td>\n",
       "      <td>He had never told us he was married</td>\n",
       "      <td>Also their navy is in a dismal state_He had ne...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>And also as used in the plant printers</td>\n",
       "      <td>Hank rusted transcended youthful tiles manslau...</td>\n",
       "      <td>And also as used in the plant printers_Hank ru...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.572328</td>\n",
       "      <td>-100.427672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>The overall growth rate of the damned thing</td>\n",
       "      <td>There are notoriously marshes and fatally simu...</td>\n",
       "      <td>The overall growth rate of the damned thing_Th...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.749828</td>\n",
       "      <td>-100.250172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>There has been huge receptions at every gate</td>\n",
       "      <td>They were to go there on a coach</td>\n",
       "      <td>There has been huge receptions at every gate_T...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>23</td>\n",
       "      <td>-100.250172</td>\n",
       "      <td>-99.749828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>There is a chemical imbalance in our brains</td>\n",
       "      <td>Under Abbot it never has and never will</td>\n",
       "      <td>There is a chemical imbalance in our brains_Un...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.144656</td>\n",
       "      <td>-100.855344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>These tables were lists of those who participate</td>\n",
       "      <td>Turnpike Gutierrez at newcomer golfers may ste...</td>\n",
       "      <td>These tables were lists of those who participa...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.572328</td>\n",
       "      <td>-100.427672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>They discuss the gambling stakes and she agrees</td>\n",
       "      <td>With Ross Smith and corners Shakespeare in London</td>\n",
       "      <td>They discuss the gambling stakes and she agree...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.749828</td>\n",
       "      <td>-100.250172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "9    A naturally occurring replies punishment exami...   \n",
       "10                   A poets piped enquiry a lot of it   \n",
       "11      A poster and employer approval rating and vows   \n",
       "37                Also their navy is in a dismal state   \n",
       "46              And also as used in the plant printers   \n",
       "..                                                 ...   \n",
       "582        The overall growth rate of the damned thing   \n",
       "594       There has been huge receptions at every gate   \n",
       "595        There is a chemical imbalance in our brains   \n",
       "596   These tables were lists of those who participate   \n",
       "597    They discuss the gambling stakes and she agrees   \n",
       "\n",
       "                                             sentence2  \\\n",
       "9    This country uses collaborative ping internal ...   \n",
       "10     The growing spectral and thus attain the bandit   \n",
       "11            The next to the time and historical park   \n",
       "37                 He had never told us he was married   \n",
       "46   Hank rusted transcended youthful tiles manslau...   \n",
       "..                                                 ...   \n",
       "582  There are notoriously marshes and fatally simu...   \n",
       "594                   They were to go there on a coach   \n",
       "595            Under Abbot it never has and never will   \n",
       "596  Turnpike Gutierrez at newcomer golfers may ste...   \n",
       "597  With Ross Smith and corners Shakespeare in London   \n",
       "\n",
       "                                      sentence_pair_id  choice_0_prob  \\\n",
       "9    A naturally occurring replies punishment exami...       0.166667   \n",
       "10   A poets piped enquiry a lot of it_The growing ...       0.500000   \n",
       "11   A poster and employer approval rating and vows...       0.666667   \n",
       "37   Also their navy is in a dismal state_He had ne...       0.500000   \n",
       "46   And also as used in the plant printers_Hank ru...       0.666667   \n",
       "..                                                 ...            ...   \n",
       "582  The overall growth rate of the damned thing_Th...       0.600000   \n",
       "594  There has been huge receptions at every gate_T...       0.400000   \n",
       "595  There is a chemical imbalance in our brains_Un...       0.800000   \n",
       "596  These tables were lists of those who participa...       0.666667   \n",
       "597  They discuss the gambling stakes and she agree...       0.600000   \n",
       "\n",
       "     choice_1_prob  subject  sentence1_raw_log_prob  sentence2_raw_log_prob  \n",
       "9         0.833333        0             -100.993023              -99.006977  \n",
       "10        0.500000        0             -100.000000             -100.000000  \n",
       "11        0.333333        0              -99.572328             -100.427672  \n",
       "37        0.500000        0             -100.000000             -100.000000  \n",
       "46        0.333333        0              -99.572328             -100.427672  \n",
       "..             ...      ...                     ...                     ...  \n",
       "582       0.400000       23              -99.749828             -100.250172  \n",
       "594       0.600000       23             -100.250172              -99.749828  \n",
       "595       0.200000       23              -99.144656             -100.855344  \n",
       "596       0.333333       23              -99.572328             -100.427672  \n",
       "597       0.400000       23              -99.749828             -100.250172  \n",
       "\n",
       "[2570 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example:\n",
    "model_sentence_predictions['lb_NC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_decision_parameters(protocol,model_predictions,decision_model_class='FixedWidthSquashing',device='cpu',optimizer='LBFGS',initial_squash_parameter=None,initial_gamma_parameter=None,max_steps_without_improvement=10,return_dataframes=True,verbose=True,param_grid=None,frozen_parameters=None):\n",
    "    \"\"\" fit the model decision hyper-parameter given a human data (protocol) and sentence predictions \n",
    "    \n",
    "    args:\n",
    "    protocol (pd.dataframe) must include a sentence_pair_id column\n",
    "    model_prediction (pd.dataframe) must include a sentence_pair_id column as well as sentence1_raw_log_prob and sentence2_raw_log_prob\n",
    "    decision_model_class (str) name of class to import from model_to_human_decision_torch. Currently implemented: 'Naive', 'NoSquashing', 'FixedWidthSquashing', 'VariableWidthSquashing', 'SquashedSoftmax'\n",
    "    device (str) torch device. cpu is faster for float64\n",
    "    optimizer (str) 'LBFGS' or 'Adam'\n",
    "    initial_squash_parameter (float) initial value for the squashing parameter \n",
    "    initial_gamma_parameter (float) initial value for the gamma parameter\n",
    "    max_steps_without_improvement (int) when to stop when not improving\n",
    "    return_dataframes (boolean) see below\n",
    "    verbose (boolean) show messages\n",
    "    param_grid (dict) if provided, uses sklearn.model_selection.ParameterGrid to return the model with the best NLL\n",
    "    frozen_parameters (list of strings) paramters to freeze (not optimize)\n",
    "    \n",
    "    returns:\n",
    "    if return_dataframes is True:\n",
    "        decision_model (ModelToHumanDecision) a fitted decision model object\n",
    "        parameters_dict (dict) the fitted parameters\n",
    "        protocol_with_NLL (pd.dataframe) the protocol with additional NLL column (trial-specific NLL)\n",
    "        adjusted_model_predictions (pd.dataframe) the model predictions with additional columns for human choice probabilities\n",
    "    otherwise:\n",
    "        decision_model, parameters_dict        \n",
    "    \"\"\"    \n",
    "    \n",
    "    # optional grid search\n",
    "    if param_grid is not None:\n",
    "        def eval_NLL_with_params(params):\n",
    "            kwargs={'decision_model_class':decision_model_class,'device':device,'optimizer':optimizer,'initial_squash_parameter':initial_squash_parameter,\n",
    "                    'initial_gamma_parameter':initial_gamma_parameter,\n",
    "                    'max_steps_without_improvement':max_steps_without_improvement,'verbose':verbose,\n",
    "                    'return_dataframes':return_dataframes,'frozen_parameters':frozen_parameters}\n",
    "            kwargs.update(params)\n",
    "            return fit_model_decision_parameters(protocol,model_predictions,**kwargs)                    \n",
    "        NLL=list()\n",
    "        for params in sklearn.model_selection.ParameterGrid(param_grid):\n",
    "            params_=params.copy()\n",
    "            params_['verbose']=False\n",
    "            params_['return_dataframes']=False\n",
    "            cur_NLL=eval_NLL_with_params(params_)[2]\n",
    "            if verbose:\n",
    "                print(params,':',cur_NLL,'NLL')\n",
    "            NLL.append(cur_NLL)\n",
    "        best_i=np.argmin(NLL)\n",
    "        best_params=sklearn.model_selection.ParameterGrid(param_grid)[best_i]\n",
    "        if verbose:\n",
    "            print('best params:',best_params)        \n",
    "        return eval_NLL_with_params(best_params)\n",
    "        \n",
    "    if 'subject' in model_predictions.columns:\n",
    "        identifiers=['subject','sentence1','sentence2','sentence_pair_id']\n",
    "    else:\n",
    "        identifiers=['sentence1','sentence2','sentence_pair_id']\n",
    "        \n",
    "    merged=protocol.merge(model_predictions,on=identifiers)  \n",
    "    \n",
    "    log_p1=torch.tensor(merged['sentence1_raw_log_prob'],device=device,dtype=torch.float64)\n",
    "    log_p2=torch.tensor(merged['sentence2_raw_log_prob'],device=device,dtype=torch.float64)\n",
    "    response_ind=torch.tensor(merged['Choice'],device=device,dtype=int)\n",
    "    \n",
    "    parameters={}\n",
    "    if initial_squash_parameter is None:\n",
    "        if decision_model_class in ['FixedWidthSquashing','VariableWidthSquashing']:\n",
    "            # get minimal sentence log-probability to serve as initial value for the squashing parameters\n",
    "            minps=abs(min(log_p1.min(),log_p2.min()))            \n",
    "            parameters['squashes']=minps\n",
    "    else:    \n",
    "            parameters['squashes']=initial_squash_parameter\n",
    "    \n",
    "    if initial_gamma_parameter is not None:\n",
    "        parameters['gamma']=initial_gamma_parameter\n",
    "    \n",
    "    # build decision model object\n",
    "    decision_model=getattr(model_to_human_decision_torch,decision_model_class)(parameters=parameters,device=device)\n",
    "    \n",
    "    for par in decision_model.parameters():\n",
    "        par.requires_grad=True\n",
    "    \n",
    "    if frozen_parameters is not None:\n",
    "        parameters_to_optimize=dict(decision_model.named_parameters())\n",
    "        for frozen_parameter in frozen_parameters:\n",
    "            del parameters_to_optimize[frozen_parameter]\n",
    "        parameters_to_optimize=parameters_to_optimize.values()\n",
    "    else:\n",
    "        parameters_to_optimize=decision_model.parameters()\n",
    "            \n",
    "    if optimizer == 'Adam':\n",
    "        opt = optim.Adam(parameters_to_optimize,lr=1.0)\n",
    "    elif optimizer=='LBFGS':\n",
    "        opt = optim.LBFGS(parameters_to_optimize,line_search_fn='strong_wolfe') # this is much faster!\n",
    "    else:\n",
    "        raise ValueError\n",
    "      \n",
    "    previous_loss=torch.tensor(np.inf,device=device)\n",
    "   \n",
    "    eps=1e-10\n",
    "    non_improvement_counter=0\n",
    "    t=time.time()\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        def calc_loss():\n",
    "            global my_loss\n",
    "            log_p_sent=decision_model(log_p1,log_p2)\n",
    "            take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "            log_p_choice=take_by_2nd_dim(log_p_sent,response_ind)\n",
    "            loss=log_p_choice.sum()     \n",
    "            return loss\n",
    "        \n",
    "        def closure(): # required for LBFGS\n",
    "            opt.zero_grad()\n",
    "            loss=calc_loss()\n",
    "            loss.backward()            \n",
    "            return loss\n",
    "    \n",
    "        opt.step(closure)\n",
    "    \n",
    "        loss=calc_loss().detach()\n",
    "        \n",
    "        if (previous_loss-loss)<eps:\n",
    "            non_improvement_counter+=1            \n",
    "        else:            \n",
    "            non_improvement_counter=0        \n",
    "        previous_loss=loss.detach()\n",
    "        \n",
    "        if non_improvement_counter>=max_steps_without_improvement:\n",
    "            if verbose:\n",
    "                print('converged in',epoch,'epochs,',time.time()-t,'seconds')\n",
    "            break\n",
    "            \n",
    "        if epoch%50==0 and verbose:            \n",
    "            print('epoch: ', epoch, 'loss:',loss.item())\n",
    "            # TODO: show parameters\n",
    "\n",
    "            #     print(decision_model.get_parameters())\n",
    "#     print('total loss:',loss)\n",
    "#     for i_model,model_name in enumerate(models):\n",
    "#         print('{} {:.2f}'.format(model_name,model_loss[i_model].item()))\n",
    "        \n",
    "    #breakpoint()\n",
    "    if verbose:\n",
    "        print(\"final loss \",loss.item())\n",
    "\n",
    "    parameters_dict=decision_model.get_parameters()\n",
    "    \n",
    "    NLL=loss.item()\n",
    "    \n",
    "    if return_dataframes:        \n",
    "        # trial-specific NLL\n",
    "        log_p_sent=decision_model(log_p1,log_p2)\n",
    "        take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "        log_p_choice=take_by_2nd_dim(log_p_sent,response_ind)\n",
    "            \n",
    "        protocol_with_NLL=protocol.copy()\n",
    "        protocol_with_NLL['NLL']=log_p_choice.detach().cpu().numpy()\n",
    "        adjusted_model_predictions=model_predictions.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            decision_log_prob=-decision_model(model_predictions.filter(regex='_raw_log_prob$',axis=1).to_numpy()).detach().cpu().numpy()\n",
    "        adjusted_model_predictions['choice_0_log_prob']=decision_log_prob[:,0]\n",
    "        adjusted_model_predictions['choice_1_log_prob']=decision_log_prob[:,1]\n",
    "        adjusted_model_predictions=adjusted_model_predictions.drop(columns=['sentence1_raw_log_prob','sentence2_raw_log_prob','choice_0_prob','choice_1_prob'],errors='ignore')        \n",
    "        return decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions\n",
    "    else:\n",
    "        return decision_model, parameters_dict, NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** fitting bert ****\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-92254270095d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdecision_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol_with_NLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_model_predictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model_decision_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_sentence_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' ---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-c3fe0ac4aace>\u001b[0m in \u001b[0;36mfit_model_decision_parameters\u001b[0;34m(protocol, model_predictions, decision_model_class, device, optimizer, initial_squash_parameter, initial_gamma_parameter, max_steps_without_improvement, return_dataframes, verbose, param_grid, frozen_parameters)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mparams_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mparams_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return_dataframes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mcur_NLL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_NLL_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_NLL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NLL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-c3fe0ac4aace>\u001b[0m in \u001b[0;36meval_NLL_with_params\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     'return_dataframes':return_dataframes,'frozen_parameters':frozen_parameters}\n\u001b[1;32m     35\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfit_model_decision_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mNLL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-c3fe0ac4aace>\u001b[0m in \u001b[0;36mfit_model_decision_parameters\u001b[0;34m(protocol, model_predictions, decision_model_class, device, optimizer, initial_squash_parameter, initial_gamma_parameter, max_steps_without_improvement, return_dataframes, verbose, param_grid, frozen_parameters)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/contstimlang/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-c3fe0ac4aace>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# required for LBFGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-c3fe0ac4aace>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mlog_p_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecision_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_p2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mtake_by_2nd_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mlog_p_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtake_by_2nd_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p_sent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponse_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/contstimlang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/contstim/model_to_human_decision_torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_p_sentences1, log_p_sentences2)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlog_p2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p_sentences2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_p2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/contstim/model_to_human_decision_torch.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(self, log_p1, log_p2)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;31m#         choice_NLL=-torch.log(torch.stack([p1,p2],dim=-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mlog_p1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms1a_b\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mlog_p2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_1_minus_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms1a_b\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mchoice_NLL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog_p1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_p2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchoice_NLL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizers=['LBFGS','Adam']\n",
    "for model in comp_models:        \n",
    "    print('**** fitting '+model + ' ****')\n",
    "    param_grid=[\n",
    "                {'decision_model_class':['FixedWidthSquashing'],'optimizer':['Adam']}, # use minps\n",
    "                {'decision_model_class':['FixedWidthSquashing'],'optimizer':['LBFGS'],}, # use minps\n",
    "                {'decision_model_class':['FixedWidthSquashing'],'optimizer':['LBFGS'],'initial_squash_parameter':np.linspace(0,300,31)},\n",
    "                #{'decision_model_class':['SquashedSoftmax'],'optimizer':optimizers},            \n",
    "                #{'decision_model_class':['FixedWidthSquashing'],'initial_squash_parameter':np.linspace(0,300,31)},                    \n",
    "                #{'decision_model_class':['VariableWidthSquashing'],'initial_squash_parameter':np.linspace(0,300,31)},\n",
    "                #{'decision_model_class':['VariableWidthSigmoid'],'initial_squash_parameter':np.linspace(0,300,31)},\n",
    "    ]\n",
    "\n",
    "    decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions[model],param_grid=param_grid,verbose=True)\n",
    "    print(parameters_dict)\n",
    "    print(' ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sanity checks:\n",
    "assert np.isclose(NLL,evaluate_NLL(protocol,adjusted_model_predictions,mode='sum'))\n",
    "assert np.isclose(NLL,protocol_with_NLL['NLL'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between decision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** fitting bert ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1571.4023663341602 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1571.2075436385578 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1565.5173837412353 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1565.5173837412353\n",
      "converged in 10 epochs, 0.0848989486694336 seconds\n",
      "final loss  1565.5173837412353\n",
      "{'gamma': 22.669362863671388, 'squashes': -4.374073384455098}\n",
      " ---\n",
      "**** fitting bert_whole_word ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1572.7144879139698 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1572.5825426301844 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1568.83091150882 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1568.83091150882\n",
      "converged in 10 epochs, 0.08283185958862305 seconds\n",
      "final loss  1568.83091150882\n",
      "{'gamma': 26.37773967620515, 'squashes': -3.8529237077746337}\n",
      " ---\n",
      "**** fitting bigram ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1562.3191163859792 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1562.3191163870338 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1560.3624454805104 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1560.3624454805104\n",
      "converged in 10 epochs, 0.08889293670654297 seconds\n",
      "final loss  1560.3624454805104\n",
      "{'gamma': 22.2896674367132, 'squashes': -7.275876341461244}\n",
      " ---\n",
      "**** fitting bilstm ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1570.516391718296 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1570.5163917988702 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1564.2242626652098 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1564.2242626652098\n",
      "converged in 10 epochs, 0.08242201805114746 seconds\n",
      "final loss  1564.2242626652098\n",
      "{'gamma': 22.976206767245323, 'squashes': -4.0391831417504145}\n",
      " ---\n",
      "**** fitting electra ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1558.4663326069217 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1558.4663326077662 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1553.322107184143 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1553.322107184143\n",
      "converged in 10 epochs, 0.07747602462768555 seconds\n",
      "final loss  1553.322107184143\n",
      "{'gamma': 22.70907833066752, 'squashes': -3.9010957421553853}\n",
      " ---\n",
      "**** fitting gpt2 ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1559.1171654186808 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1557.8934938366492 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1554.866205093827 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1554.866205093827\n",
      "converged in 10 epochs, 0.07832670211791992 seconds\n",
      "final loss  1554.866205093827\n",
      "{'gamma': 23.710766043078475, 'squashes': -4.027735109878557}\n",
      " ---\n",
      "**** fitting lstm ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1572.4363987461275 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1562.9181283768369 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1555.1082232327792 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1555.1082232327792\n",
      "converged in 10 epochs, 0.0999302864074707 seconds\n",
      "final loss  1555.1082232327792\n",
      "{'gamma': 24.102962008102438, 'squashes': -3.6639694072535454}\n",
      " ---\n",
      "**** fitting rnn ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1587.3529176526847 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1571.7694177123376 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1569.6613433861287 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1569.6613433861287\n",
      "converged in 10 epochs, 0.07927465438842773 seconds\n",
      "final loss  1569.6613433861287\n",
      "{'gamma': 23.76740230289882, 'squashes': -3.7020861415349304}\n",
      " ---\n",
      "**** fitting roberta ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1562.3068851598036 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1562.2582032340263 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1553.8485733491855 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1553.8485733491855\n",
      "converged in 10 epochs, 0.07945919036865234 seconds\n",
      "final loss  1553.8485733491855\n",
      "{'gamma': 20.926074111158457, 'squashes': -4.681458197169742}\n",
      " ---\n",
      "**** fitting trigram ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1566.9911817659076 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1566.9911817668167 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1566.7094030882254 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1566.7094031107347\n",
      "converged in 11 epochs, 0.09659409523010254 seconds\n",
      "final loss  1566.7094030882254\n",
      "{'gamma': 27.87823674044754, 'squashes': -6.53053758575376}\n",
      " ---\n",
      "**** fitting xlm ****\n",
      "{'decision_model_class': 'NoSquashing', 'optimizer': 'LBFGS'} : 1570.7725019701245 NLL\n",
      "{'decision_model_class': 'FixedWidthSquashing', 'optimizer': 'LBFGS'} : 1570.4252271455293 NLL\n",
      "{'decision_model_class': 'SquashedSoftmax', 'optimizer': 'LBFGS'} : 1566.5096281340302 NLL\n",
      "best params: {'optimizer': 'LBFGS', 'decision_model_class': 'SquashedSoftmax'}\n",
      "epoch:  0 loss: 1566.5096281340302\n",
      "converged in 10 epochs, 0.08584856986999512 seconds\n",
      "final loss  1566.5096281340302\n",
      "{'gamma': 25.41556472778878, 'squashes': -3.9150763362562992}\n",
      " ---\n"
     ]
    }
   ],
   "source": [
    "optimizers=['LBFGS']\n",
    "for model in comp_models:        \n",
    "    print('**** fitting '+model + ' ****')\n",
    "    param_grid=[                \n",
    "                {'decision_model_class':['NoSquashing'],'optimizer':optimizers,}, # use minps\n",
    "                {'decision_model_class':['FixedWidthSquashing'],'optimizer':optimizers,}, # use minps\n",
    "                {'decision_model_class':['SquashedSoftmax'],'optimizer':optimizers},      \n",
    "        \n",
    "                #{'decision_model_class':['FixedWidthSquashing'],'initial_squash_parameter':np.linspace(0,300,31)},                    \n",
    "                #{'decision_model_class':['VariableWidthSquashing'],'initial_squash_parameter':np.linspace(0,300,31)},\n",
    "                #{'decision_model_class':['VariableWidthSigmoid'],'initial_squash_parameter':np.linspace(0,300,31)},\n",
    "    ]\n",
    "\n",
    "    decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions[model],param_grid=param_grid,verbose=True)\n",
    "    print(parameters_dict)\n",
    "    print(' ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert SquashedSoftmax NLL: 1565.5173837412353 parameters: {'gamma': 22.669362863671388, 'squashes': -4.374073384455098}\n",
      "bert_whole_word SquashedSoftmax NLL: 1568.83091150882 parameters: {'gamma': 26.37773967620515, 'squashes': -3.8529237077746337}\n",
      "bigram SquashedSoftmax NLL: 1560.3624454805104 parameters: {'gamma': 22.2896674367132, 'squashes': -7.275876341461244}\n",
      "bilstm SquashedSoftmax NLL: 1564.2242626652098 parameters: {'gamma': 22.976206767245323, 'squashes': -4.0391831417504145}\n",
      "electra SquashedSoftmax NLL: 1553.322107184143 parameters: {'gamma': 22.70907833066752, 'squashes': -3.9010957421553853}\n",
      "gpt2 SquashedSoftmax NLL: 1554.866205093827 parameters: {'gamma': 23.710766043078475, 'squashes': -4.027735109878557}\n",
      "lstm SquashedSoftmax NLL: 1555.1082232327792 parameters: {'gamma': 24.102962008102438, 'squashes': -3.6639694072535454}\n",
      "rnn SquashedSoftmax NLL: 1569.6613433861287 parameters: {'gamma': 23.76740230289882, 'squashes': -3.7020861415349304}\n",
      "roberta SquashedSoftmax NLL: 1553.8485733491855 parameters: {'gamma': 20.926074111158457, 'squashes': -4.681458197169742}\n",
      "trigram SquashedSoftmax NLL: 1566.7094030882254 parameters: {'gamma': 27.87823674044754, 'squashes': -6.53053758575376}\n",
      "xlm SquashedSoftmax NLL: 1566.5096281340302 parameters: {'gamma': 25.41556472778878, 'squashes': -3.9150763362562992}\n",
      "bert FixedWidthSquashing NLL: 1571.2075436385587 parameters: {'squashes': 131.26903725184218, 'gamma': 36.056265140456155}\n",
      "bert_whole_word FixedWidthSquashing NLL: 1572.5825426301687 parameters: {'squashes': 134.49532662620712, 'gamma': 39.41406266358603}\n",
      "bigram FixedWidthSquashing NLL: 1562.3191163859792 parameters: {'squashes': 200.00000000000014, 'gamma': 29.803726742307155}\n",
      "bilstm FixedWidthSquashing NLL: 1569.533531585994 parameters: {'squashes': 115.53088298382605, 'gamma': 35.83830068356804}\n",
      "electra FixedWidthSquashing NLL: 1558.4663326069217 parameters: {'squashes': 160.00000000000185, 'gamma': 35.420503489811395}\n",
      "gpt2 FixedWidthSquashing NLL: 1557.8934938366483 parameters: {'squashes': 106.22758775449482, 'gamma': 32.99583004345502}\n",
      "lstm FixedWidthSquashing NLL: 1562.918128376825 parameters: {'squashes': 111.68748341572149, 'gamma': 38.09929738714122}\n",
      "rnn FixedWidthSquashing NLL: 1571.769417712314 parameters: {'squashes': 108.33739117910302, 'gamma': 36.46689780244632}\n",
      "roberta FixedWidthSquashing NLL: 1561.2284035699095 parameters: {'squashes': 113.77922526792442, 'gamma': 31.517380135520263}\n",
      "trigram FixedWidthSquashing NLL: 1566.9911817659076 parameters: {'squashes': 190.0, 'gamma': 31.361804462000833}\n",
      "xlm FixedWidthSquashing NLL: 1570.4252271452629 parameters: {'squashes': 133.63687283386088, 'gamma': 38.133852160528086}\n",
      "bert NoSquashing NLL: 1571.4023663341602 parameters: {'gamma': 36.37450390256927}\n",
      "bert_whole_word NoSquashing NLL: 1572.7144879139698 parameters: {'gamma': 39.491986788955096}\n",
      "bigram NoSquashing NLL: 1562.3191163859792 parameters: {'gamma': 29.803726742307166}\n",
      "bilstm NoSquashing NLL: 1570.516391718296 parameters: {'gamma': 37.408148473532584}\n",
      "electra NoSquashing NLL: 1558.4663326069217 parameters: {'gamma': 35.4205034898114}\n",
      "gpt2 NoSquashing NLL: 1559.1171654186808 parameters: {'gamma': 34.801912864949614}\n",
      "lstm NoSquashing NLL: 1572.4363987461275 parameters: {'gamma': 50.333846975795346}\n",
      "rnn NoSquashing NLL: 1587.3529176526847 parameters: {'gamma': 50.08273982290296}\n",
      "roberta NoSquashing NLL: 1562.3068851598036 parameters: {'gamma': 36.16476206614194}\n",
      "trigram NoSquashing NLL: 1566.9911817659076 parameters: {'gamma': 31.36180446200083}\n",
      "xlm NoSquashing NLL: 1570.7725019701245 parameters: {'gamma': 38.22366873155245}\n"
     ]
    }
   ],
   "source": [
    "# train and save decision models for all models\n",
    "optimizer='LBFGS'\n",
    "sent_len=8\n",
    "\n",
    "# train and save models\n",
    "for decision_model_class in ['SquashedSoftmax','FixedWidthSquashing','NoSquashing']:\n",
    "    for model in comp_models:\n",
    "        if decision_model_class=='FixedWidthSquashing':\n",
    "            param_grid=[{'initial_squash_parameter':np.linspace(0,300,31)}]\n",
    "        else:\n",
    "            param_grid=None\n",
    "        decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions[model],decision_model_class=decision_model_class,optimizer=optimizer,verbose=False,param_grid=param_grid)\n",
    "        path = os.path.join('decision_models','20210115',decision_model_class+\"_\" +optimizer+\"_{}_word\".format(sent_len),model+'.pkl')\n",
    "        print(model,decision_model_class,'NLL:',NLL,'parameters:',parameters_dict)\n",
    "        decision_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert FixedWidthSquashing NLL: 1658.2361335345831 parameters: {'squashes': 62.71453811046773, 'gamma': 11.384543899159606}\n",
      "bert_whole_word FixedWidthSquashing NLL: 1652.2034728378417 parameters: {'squashes': 56.92383811131959, 'gamma': 12.950105725448667}\n",
      "bigram FixedWidthSquashing NLL: 1643.8402985992554 parameters: {'squashes': 121.51904296875, 'gamma': 12.341410286904521}\n",
      "bilstm FixedWidthSquashing NLL: 1656.0935858463858 parameters: {'squashes': 57.18711783290085, 'gamma': 11.845979709762771}\n",
      "electra FixedWidthSquashing NLL: 1647.5160489652053 parameters: {'squashes': 49.83518087675971, 'gamma': 11.127596031582593}\n",
      "gpt2 FixedWidthSquashing NLL: 1675.619378669754 parameters: {'squashes': 48.35059190027987, 'gamma': 10.512459597557543}\n",
      "lstm FixedWidthSquashing NLL: 1644.7726118109154 parameters: {'squashes': 56.265304489099634, 'gamma': 12.779863227391383}\n",
      "rnn FixedWidthSquashing NLL: 1653.0387932698545 parameters: {'squashes': 60.962956533614665, 'gamma': 14.849769912840127}\n",
      "roberta FixedWidthSquashing NLL: 1654.5517787470171 parameters: {'squashes': 64.56249312197863, 'gamma': 10.041136791736587}\n",
      "trigram FixedWidthSquashing NLL: 1653.6429142083023 parameters: {'squashes': 106.857666015625, 'gamma': 13.6972062134497}\n",
      "xlm FixedWidthSquashing NLL: 1654.0351139079128 parameters: {'squashes': 56.94718881731906, 'gamma': 12.886847644025059}\n"
     ]
    }
   ],
   "source": [
    "# train and save FixedWidthSquashing decision models using percentile based squashing threshold\n",
    "optimizer='LBFGS'\n",
    "sent_len=8\n",
    "\n",
    "which_percentile_to_use=10\n",
    "model_logprob_percentiles_dict=pickle.load(open('model_logprob_percentiles.pkl','rb'))\n",
    "\n",
    "# train and save models\n",
    "for decision_model_class in ['FixedWidthSquashing']:\n",
    "    for model in comp_models:        \n",
    "        squashing_threshold=abs(model_logprob_percentiles_dict[model][which_percentile_to_use])\n",
    "        decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(\n",
    "            protocol,model_sentence_predictions[model],decision_model_class=decision_model_class,\n",
    "            optimizer=optimizer,verbose=False,\n",
    "            frozen_parameters=['squashes'],initial_squash_parameter=squashing_threshold)\n",
    "        path = os.path.join('decision_models','20210118_10th_percentile_squash',decision_model_class+\"_\" +optimizer+\"_{}_word\".format(sent_len),model+'.pkl')\n",
    "        print(model,decision_model_class,'NLL:',NLL,'parameters:',parameters_dict)\n",
    "        decision_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parameters_across_models(decision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gT=np.array(gamma_boots).T\n",
    "sT=np.array(squash_boots).T\n",
    "\n",
    "ccs=[]\n",
    "for i in range(11):\n",
    "    \n",
    "    cc=np.corrcoef(sT[i,:],gT[i,:])[0,1]\n",
    "    ccs.append(cc)\n",
    "    \n",
    "ccs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synthetic\n",
    "\n",
    "# tensor(671.4779, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.5626, 128.7844,  84.1193,  92.4041, 111.8108, 105.3416,  83.8154,\n",
    "#         116.5818,  87.0363,  60.6492,  81.0560], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([29.8585, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "#natural\n",
    "\n",
    "# tensor(148.7837, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([115.8415,  84.2844,  72.0876,  43.8699,  54.9998,  50.4821,  69.6983,\n",
    "#          49.1065,  42.4528,  34.6840,  59.5816], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([ 9.1418, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "# tensor(829.6835, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.1968, 127.7177,  83.4297,  91.0015, 111.4391, 104.3089,  83.3092,\n",
    "#         115.3282,  85.6379,  59.8025,  79.7266], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([28.2664, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squash_boots=np.array([[136.6031, 130.6315,  88.2857, 127.1014, 124.6718, 110.4878,  84.6417,\n",
    "#         118.1435, 107.7737,  63.0790,  79.9143],\n",
    "#               [137.9003, 125.7105,  84.5635, 127.1772, 112.0871,  94.5774,  78.6344,\n",
    "#         114.0863,  95.4273,  58.2541,  79.6323],\n",
    "#               [131.8761, 133.7181, 147.7142, 143.2530, 110.6293, 134.2203,  83.7578,\n",
    "#         139.0220,  82.7407,  66.7261,  79.2919],\n",
    "#               [139.6443, 135.6945,  80.5810,  94.8058, 116.6113, 103.0531,  87.4392,\n",
    "#         119.7775,  81.2322,  60.9193,  81.1773],\n",
    "#               [132.4967, 128.7608,  92.4549,  91.8489, 110.4346, 104.1544,  80.5317,\n",
    "#         126.0785, 124.9601,  57.4880,  76.9285],\n",
    "#               [128.2988, 112.9426,  80.0811,  90.9844, 111.0652, 115.8396,  56.3730,\n",
    "#          78.9889, 120.2621,  29.6331,  61.9582],\n",
    "#               [127.7181, 132.9688, 148.8201,  94.7272, 109.7225,  87.5404,  81.3132,\n",
    "#         115.8013, 116.9241,  64.7583,  73.5550],\n",
    "#               [120.8540, 116.8421, 135.4461,  88.5892, 124.1079, 133.7935,  86.1704,\n",
    "#         100.1683,  92.7386,  49.7565,  79.4477],\n",
    "#               [125.4391, 114.5180,  82.5459,  87.4416, 110.8198,  93.8739,  63.4568,\n",
    "#         117.9814,  83.9064,  57.2624,  77.2107],\n",
    "#               [129.6447, 129.3590,  86.6444,  92.5276, 109.5141, 105.0256,  65.8152,\n",
    "#         117.5682, 125.8277,  59.4612,  77.8789]])\n",
    "\n",
    "s=squash_boots\n",
    "\n",
    "squash_boots=np.array(losses).T\n",
    "squash_boots.sort()\n",
    "squash_boots=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squash_boots=squash_boots.reshape([10,11])\n",
    "\n",
    "\n",
    "\n",
    "x=np.arange(11)*2\n",
    "# plt.bar(x-0.2, maxps, width=0.1, color=[1,0,0])\n",
    "plt.bar(x-0.1, squash_boots[0], width=0.1, color=[0,0,1])\n",
    "plt.bar(x, squash_boots[1], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.1, squash_boots[2], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.2, squash_boots[3], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.3, squash_boots[4], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.4, squash_boots[5], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.5, squash_boots[6], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.6, squash_boots[7], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.7, squash_boots[8], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.8, squash_boots[9], width=0.1, color=[0,0,1])\n",
    "# plt.bar(x+0.9, minps, width=0.1, color=[1,0,0])\n",
    "\n",
    "# plt.ylabel('Squash Thresholds')\n",
    "plt.ylabel('Negative log likelihood')\n",
    "plt.xticks(x,models,rotation=70)\n",
    "\n",
    "# plt.ylim([800,950])\n",
    "\n",
    "legs=['Min/Max baseline','Squash thresholds']\n",
    "# plt.legend(legs,bbox_to_anchor=(1, 0.3, 0.2, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=np.array([f.sort() for f in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots=d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist=list(set(all_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('test_sents_expt1_first13subs.txt','w')\n",
    "for s in slist:\n",
    "    file.write(s)\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model.parameters['squashes'].data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent1,sent2].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=0\n",
    "a=0\n",
    "for trial in fitset:\n",
    "    \n",
    "    if trial[0]=='electra':\n",
    "        \n",
    "        a+=1\n",
    "    \n",
    "        pa=trial[1]\n",
    "        pb=trial[2]\n",
    "        c=trial[3]\n",
    "\n",
    "        if c==0 and pa>pb:\n",
    "            g+=1\n",
    "        elif c==1 and pb>pa:\n",
    "            g+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g/len(fitset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(item_prob_ceilings[item_id][1])==-math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
