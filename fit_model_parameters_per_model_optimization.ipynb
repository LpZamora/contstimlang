{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, pickle, time, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.dpi']= 200\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import model_to_human_decision_torch \n",
    "from model_to_human_decision_torch import get_parameters_across_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data_csv='data_exp1_cumulative.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tal/anaconda3/envs/contstimlang/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 70 within-subject repeating trials.\n"
     ]
    }
   ],
   "source": [
    "def prepare_tidy_protocol(csv_path,do_remove_within_subject_repetitions=True):\n",
    "    df=pd.read_csv(csv_path)\n",
    "    \n",
    "    df=df.loc[df.Response.notna(),:] #drop non-response lines\n",
    "    df=df.rename(columns={'source':'source_set 1'}) # fix a mistake in model naming\n",
    "    df=df.rename(columns={'counterbalance-o1ql':'set_num'})\n",
    "\n",
    "    # transform set columns to trial columns\n",
    "    p = re.compile('(.+)_set \\d+')\n",
    "    set_columns=[s for s in list(df.columns) if p.match(s)]\n",
    "    columns_to_keep=['Participant External Session ID','Event Index','Reaction Time','Response','set_num']+set_columns\n",
    "    columns_to_drop=list(np.setdiff1d(df.columns,columns_to_keep))\n",
    "    df=df.drop(columns=columns_to_drop)\n",
    "    columns_to_build=list(np.unique([p.findall(s)[0] for s in set_columns]))\n",
    "    for index,row in df.iterrows():\n",
    "        cur_set=row['set_num']\n",
    "        for c in columns_to_build:\n",
    "            df.loc[index,c]=df.loc[index,c+'_'+cur_set]\n",
    "    df=df.drop(columns=set_columns+['set_num'])\n",
    "\n",
    "    # make sure the sentence pairs are alphabetically sorted\n",
    "    def sort_sentence_pairs(df1):    \n",
    "        flip_dict={'sentence1':'sentence2','sentence2':'sentence1',\n",
    "                   'sentence1_model':'sentence2_model','sentence2_model':'sentence1_model'}\n",
    "        df2=df1.copy()    \n",
    "        for index, row in df2.iterrows():\n",
    "            if row['sentence1']>row['sentence2']: # a flip is needed\n",
    "                for old_col, new_col in flip_dict.items():\n",
    "                    df2.loc[index,new_col]=df1.loc[index,old_col]\n",
    "        return df2\n",
    "    df=sort_sentence_pairs(df)\n",
    "\n",
    "    # separate models and levels\n",
    "    p=re.compile('(.+)_(\\d+)')\n",
    "    df['sentence1_model_name']=[p.findall(s)[0][0] for s in df['sentence1_model']]\n",
    "    df['sentence2_model_name']=[p.findall(s)[0][0] for s in df['sentence2_model']]\n",
    "    df['sentence1_model_level']=[int(p.findall(s)[0][1]) for s in df['sentence1_model']]\n",
    "    df['sentence2_model_level']=[int(p.findall(s)[0][1]) for s in df['sentence2_model']]\n",
    "    df=df.drop(columns=['sentence1_model','sentence2_model'])\n",
    "\n",
    "    # mark choice indecis\n",
    "    for index,row in df.iterrows():\n",
    "        df.loc[index,'Choice']=[row.sentence1, row.sentence2].index(row.Response)\n",
    "\n",
    "    df['sentence_pair_id']=[s1+'_'+s2 for s1,s2 in zip(df.sentence1, df.sentence2)]\n",
    "\n",
    "    # renumber subjects\n",
    "    df.drop(columns='subject') # there's some issue here    \n",
    "    uq_subjects, ind, unique_inverse = np.unique(df['Participant External Session ID'], return_index=True,return_inverse=True)\n",
    "    uq_subjects=list(uq_subjects[np.argsort(ind)]) # unique subjects by order of appearance    \n",
    "    df['subject']=[uq_subjects.index(s) for s in df['Participant External Session ID']]\n",
    "                   \n",
    "    # optionally, remove all but first occurance of each sentence pair within a subject    \n",
    "    if do_remove_within_subject_repetitions:\n",
    "        def remove_within_subject_repetitions(df1):\n",
    "            # keep only the first appearance of each sentence pair within a subject\n",
    "            _,unique_indices=np.unique(df1.sentence_pair_id,return_index=True)\n",
    "            df2=df1.loc[np.in1d(np.arange(len(df1)),unique_indices),:]    \n",
    "            return df2\n",
    "        old_length=len(df)\n",
    "        df=df.groupby('subject').apply(remove_within_subject_repetitions)\n",
    "        print(\"removed {} within-subject repeating trials.\".format(old_length-len(df)))\n",
    "        \n",
    "    df=df.set_index(['subject','trial']).sort_values(by=['subject','trial'],axis=0).reset_index()\n",
    "    \n",
    "    # a final sanity check: \n",
    "    assert len(np.unique(df['sentence_pair_id']))==len(df.drop_duplicates(subset=['sentence1','sentence2'])), 'number of sentences pairs must match number of sentence pair ids'\n",
    "    \n",
    "    return df\n",
    "protocol=prepare_tidy_protocol(task_data_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2570 total trials with 605 sentence pairs and 24 subjects.\n"
     ]
    }
   ],
   "source": [
    "n_total_trials=len(protocol)\n",
    "n_sentence_pairs=len(np.unique(protocol['sentence_pair_id']))\n",
    "n_subjects=len(np.unique(protocol['Participant External Session ID']))\n",
    "print(\"loaded {} total trials with {} sentence pairs and {} subjects.\".format(n_total_trials,n_sentence_pairs,n_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial</th>\n",
       "      <th>Event Index</th>\n",
       "      <th>Participant External Session ID</th>\n",
       "      <th>Reaction Time</th>\n",
       "      <th>Response</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence1_model_name</th>\n",
       "      <th>sentence2_model_name</th>\n",
       "      <th>sentence1_model_level</th>\n",
       "      <th>sentence2_model_level</th>\n",
       "      <th>Choice</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>5812.220000</td>\n",
       "      <td>What public schools teach their students is co...</td>\n",
       "      <td>I can not long conceal my dreadful sin</td>\n",
       "      <td>What public schools teach their students is co...</td>\n",
       "      <td>internet</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I can not long conceal my dreadful sin_What pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>5300.000000</td>\n",
       "      <td>And that consists of a number of factors</td>\n",
       "      <td>And that consists of a number of factors</td>\n",
       "      <td>Birmingham but the heavy duty cell one and</td>\n",
       "      <td>generator</td>\n",
       "      <td>electra</td>\n",
       "      <td>electra</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And that consists of a number of factors_Birmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>9964.815000</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubts</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubts</td>\n",
       "      <td>Mel causal that foothold overflowed paid Cpl a...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dad of a stagnant fermentation shrimp of doubt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>11878.520000</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "      <td>The frustrating and bananas and the modificati...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bigram</td>\n",
       "      <td>bigram</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Brisbane corp distant and finished commandos c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5f89a28900ebf4068239ad58</td>\n",
       "      <td>12686.870000</td>\n",
       "      <td>Listening those studied bails heater understan...</td>\n",
       "      <td>Hours into cults cables help arrive in mounts</td>\n",
       "      <td>Listening those studied bails heater understan...</td>\n",
       "      <td>generator</td>\n",
       "      <td>lstm</td>\n",
       "      <td>lstm</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hours into cults cables help arrive in mounts_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>214</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>1467.500000</td>\n",
       "      <td>List of people in America who hate it</td>\n",
       "      <td>Blog negative blog shelve grammar represents c...</td>\n",
       "      <td>List of people in America who hate it</td>\n",
       "      <td>generator</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blog negative blog shelve grammar represents c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>23</td>\n",
       "      <td>107.0</td>\n",
       "      <td>216</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2621.625001</td>\n",
       "      <td>Lions in his most charming little extraterrest...</td>\n",
       "      <td>Forces bucks hometown donors Berger complicati...</td>\n",
       "      <td>Lions in his most charming little extraterrest...</td>\n",
       "      <td>generator</td>\n",
       "      <td>trigram</td>\n",
       "      <td>trigram</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forces bucks hometown donors Berger complicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>23</td>\n",
       "      <td>108.0</td>\n",
       "      <td>218</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2137.399999</td>\n",
       "      <td>Immediately after completing algorithms the ga...</td>\n",
       "      <td>Danny Louvre shrouded typically flees offendin...</td>\n",
       "      <td>Immediately after completing algorithms the ga...</td>\n",
       "      <td>generator</td>\n",
       "      <td>roberta</td>\n",
       "      <td>roberta</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Danny Louvre shrouded typically flees offendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>23</td>\n",
       "      <td>109.0</td>\n",
       "      <td>220</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>1881.740000</td>\n",
       "      <td>The man sleeps with the second solid object</td>\n",
       "      <td>Hart bitter motions Toledo going whacking emer...</td>\n",
       "      <td>The man sleeps with the second solid object</td>\n",
       "      <td>generator</td>\n",
       "      <td>xlm</td>\n",
       "      <td>xlm</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hart bitter motions Toledo going whacking emer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>23</td>\n",
       "      <td>110.0</td>\n",
       "      <td>222</td>\n",
       "      <td>5fde554b85dbbb5cf9a1d168</td>\n",
       "      <td>2625.890001</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "      <td>Want sits preaching subscription as spotlights...</td>\n",
       "      <td>generator</td>\n",
       "      <td>bigram</td>\n",
       "      <td>bigram</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Anyone electron elevation ponies circulate the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject  trial Event Index Participant External Session ID  \\\n",
       "0           0    1.0           4        5f89a28900ebf4068239ad58   \n",
       "1           0    2.0           6        5f89a28900ebf4068239ad58   \n",
       "2           0    3.0           8        5f89a28900ebf4068239ad58   \n",
       "3           0    4.0          10        5f89a28900ebf4068239ad58   \n",
       "4           0    5.0          12        5f89a28900ebf4068239ad58   \n",
       "...       ...    ...         ...                             ...   \n",
       "2565       23  106.0         214        5fde554b85dbbb5cf9a1d168   \n",
       "2566       23  107.0         216        5fde554b85dbbb5cf9a1d168   \n",
       "2567       23  108.0         218        5fde554b85dbbb5cf9a1d168   \n",
       "2568       23  109.0         220        5fde554b85dbbb5cf9a1d168   \n",
       "2569       23  110.0         222        5fde554b85dbbb5cf9a1d168   \n",
       "\n",
       "      Reaction Time                                           Response  \\\n",
       "0       5812.220000  What public schools teach their students is co...   \n",
       "1       5300.000000           And that consists of a number of factors   \n",
       "2       9964.815000    Dad of a stagnant fermentation shrimp of doubts   \n",
       "3      11878.520000  Brisbane corp distant and finished commandos c...   \n",
       "4      12686.870000  Listening those studied bails heater understan...   \n",
       "...             ...                                                ...   \n",
       "2565    1467.500000              List of people in America who hate it   \n",
       "2566    2621.625001  Lions in his most charming little extraterrest...   \n",
       "2567    2137.399999  Immediately after completing algorithms the ga...   \n",
       "2568    1881.740000        The man sleeps with the second solid object   \n",
       "2569    2625.890001  Anyone electron elevation ponies circulate the...   \n",
       "\n",
       "                                              sentence1  \\\n",
       "0                I can not long conceal my dreadful sin   \n",
       "1              And that consists of a number of factors   \n",
       "2       Dad of a stagnant fermentation shrimp of doubts   \n",
       "3     Brisbane corp distant and finished commandos c...   \n",
       "4         Hours into cults cables help arrive in mounts   \n",
       "...                                                 ...   \n",
       "2565  Blog negative blog shelve grammar represents c...   \n",
       "2566  Forces bucks hometown donors Berger complicati...   \n",
       "2567  Danny Louvre shrouded typically flees offendin...   \n",
       "2568  Hart bitter motions Toledo going whacking emer...   \n",
       "2569  Anyone electron elevation ponies circulate the...   \n",
       "\n",
       "                                              sentence2     source  \\\n",
       "0     What public schools teach their students is co...   internet   \n",
       "1            Birmingham but the heavy duty cell one and  generator   \n",
       "2     Mel causal that foothold overflowed paid Cpl a...  generator   \n",
       "3     The frustrating and bananas and the modificati...  generator   \n",
       "4     Listening those studied bails heater understan...  generator   \n",
       "...                                                 ...        ...   \n",
       "2565              List of people in America who hate it  generator   \n",
       "2566  Lions in his most charming little extraterrest...  generator   \n",
       "2567  Immediately after completing algorithms the ga...  generator   \n",
       "2568        The man sleeps with the second solid object  generator   \n",
       "2569  Want sits preaching subscription as spotlights...  generator   \n",
       "\n",
       "     sentence1_model_name sentence2_model_name  sentence1_model_level  \\\n",
       "0                     xlm                  xlm                      1   \n",
       "1                 electra              electra                      9   \n",
       "2                  bilstm               bilstm                      5   \n",
       "3                  bigram               bigram                      2   \n",
       "4                    lstm                 lstm                      5   \n",
       "...                   ...                  ...                    ...   \n",
       "2565                  xlm                  xlm                      3   \n",
       "2566              trigram              trigram                      1   \n",
       "2567              roberta              roberta                      1   \n",
       "2568                  xlm                  xlm                      1   \n",
       "2569               bigram               bigram                      1   \n",
       "\n",
       "      sentence2_model_level  Choice  \\\n",
       "0                         0     1.0   \n",
       "1                         5     0.0   \n",
       "2                         0     0.0   \n",
       "3                         6     0.0   \n",
       "4                         2     1.0   \n",
       "...                     ...     ...   \n",
       "2565                      9     1.0   \n",
       "2566                      5     1.0   \n",
       "2567                      6     1.0   \n",
       "2568                      8     1.0   \n",
       "2569                      2     0.0   \n",
       "\n",
       "                                       sentence_pair_id  \n",
       "0     I can not long conceal my dreadful sin_What pu...  \n",
       "1     And that consists of a number of factors_Birmi...  \n",
       "2     Dad of a stagnant fermentation shrimp of doubt...  \n",
       "3     Brisbane corp distant and finished commandos c...  \n",
       "4     Hours into cults cables help arrive in mounts_...  \n",
       "...                                                 ...  \n",
       "2565  Blog negative blog shelve grammar represents c...  \n",
       "2566  Forces bucks hometown donors Berger complicati...  \n",
       "2567  Danny Louvre shrouded typically flees offendin...  \n",
       "2568  Hart bitter motions Toledo going whacking emer...  \n",
       "2569  Anyone electron elevation ponies circulate the...  \n",
       "\n",
       "[2570 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate negative log-likelihood of choice predictions given as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NLL(subject_data,predictions,mode='sum',deal_with_missing_predictions='omit',return_non_agg_NLL=False):\n",
    "    \"\"\" evaluate the negative-log-likelihood of human response data given model predictions.\n",
    "    \n",
    "    subject_data (pd.DataFrame) a dataframe such as the one returned from prepare_tidy_protocol. Should contain 'subject' (str), 'sentence_pair_id' (str) and 'Choice' (either 0 or 1).\n",
    "    predictions (pd.DataFrame) model predictions. \n",
    "        Should contain sentence_pair_id (str), and either 'choice_0_prob' and 'choice_1_prob' or 'choice_0_log_prob' and 'choice_1_log_prob': \n",
    "        the (log) probabilitiy of choosing sentence 0 or the other one.\n",
    "        the predictions dataframe can contain also a 'subject' column for subject specific-predictions\n",
    "    mode (str) 'sum' or 'mean' (how NLL is aggregeated).\n",
    "    deal_with_missing_predictions (str) 'omit' - deletes trials with no predictions. uniform predictions - uses a p=0.5 prediction.\n",
    "    return_non_agg_NLL (boolean)\n",
    "    \n",
    "    returns:\n",
    "    if return_non_agg_NLL is True, (aggregated dataframe, original dataframe)\n",
    "    else, only the aggregated dataframe\n",
    "    both return dataframes contain a new columns NLL - negative-log-likelihood\n",
    "    \n",
    "    \"\"\"\n",
    "    if 'subject' in predictions.columns: # predictions are subject-specific\n",
    "        df=subject_data.merge(predictions,how='left',on=['sentence_pair_id','subject'])\n",
    "    else: # same predictions for all subjects\n",
    "        df=subject_data.merge(predictions,how='left',on=['sentence_pair_id'])\n",
    "    c=np.asarray(subject_data.Choice,dtype=float)\n",
    "    \n",
    "    if 'choice_0_log_prob' in predictions.columns and 'choice_1_log_prob' in predictions.columns:\n",
    "        log_p=np.where(c==0,df.choice_0_log_prob,df.choice_1_log_prob)\n",
    "        log_p=np.where(np.isnan(c),np.nan,log_p)\n",
    "    elif 'choice_0_prob' in predictions.columns and 'choice_1_prob' in predictions.columns:\n",
    "        p=np.where(c==0,df.choice_0_prob,df.choice_1_prob)\n",
    "        p=np.where(np.isnan(c),np.nan,p)\n",
    "        log_p=np.log(p)\n",
    "    else:\n",
    "        raise ValueError('predictions must include either choice_0_log_prob and choice_1_log_prob or choice_0_prob and choice_1_prob')\n",
    "    \n",
    "    df['NLL']=-log_p\n",
    "    \n",
    "    # deal with trials for which we have no predictions:\n",
    "    missing_predictions=np.logical_and(df['NLL'].isna(),np.logical_not(np.isnan(c))) # there's no prediction but the subject made a choice\n",
    "    \n",
    "    if np.any(missing_predictions):\n",
    "        print('Found {} missing predictions. Handling strategy: {}.'.format(np.sum(missing_predictions),deal_with_missing_predictions))\n",
    "        if deal_with_missing_predictions=='omit':\n",
    "            df=df.loc[np.logical_not(missing_predictions),:] # drop these trails\n",
    "        elif deal_with_missing_predictions=='uniform predictions':\n",
    "            df[missing_predictions,'NLL']=-np.log(0.5) # make a uniform prediction\n",
    "        else:\n",
    "            raise ValueError(\"invalid deal_with_missing_predictions\")\n",
    "    \n",
    "    # aggregate NLL\n",
    "    if mode=='sum':\n",
    "        agg_NLL=df.NLL.sum().item()        \n",
    "    elif mode=='mean':\n",
    "        agg_NLL=df.NLL.mean().item()\n",
    "    if return_non_agg_NLL:\n",
    "        return agg_NLL, df\n",
    "    else:\n",
    "        return agg_NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper and lower bounds on the noise ceiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper noise ceiling NLL 1142.902361691926\n"
     ]
    }
   ],
   "source": [
    "def get_ub_noise_ceiling_predictions(subject_data):    \n",
    "    # calculate human choice probability per sentence (collapsed over subjects and within-subject repetitions)\n",
    "    df=subject_data.copy()\n",
    "    df['chose_0']=np.asarray(subject_data.Choice==0,np.float)\n",
    "    df['chose_0'].loc[np.isnan(subject_data.Choice)]==np.nan\n",
    "    df2=df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].mean()\n",
    "    df2=df2.drop(columns=['subject']).reset_index().rename(columns={'chose_0':'choice_0_prob'})\n",
    "    df2['choice_1_prob']=1.0-np.asarray(df2.choice_0_prob)\n",
    "    return df2\n",
    "ub_NC_predictions=get_ub_noise_ceiling_predictions(protocol)\n",
    "\n",
    "def get_lb_noise_ceiling_predictions(subject_data):\n",
    "    # calculate leave-one-subject out human choice probability per sentence (collapsed over subjects and within-subject repetitions)\n",
    "    subjects=list(np.unique(subject_data.subject))\n",
    "    df1=subject_data.copy()\n",
    "    df1['chose_0']=np.asarray(df1.Choice==0,np.float)\n",
    "    df1['chose_0'].loc[np.isnan(df1.Choice)]==np.nan\n",
    "    \n",
    "    subject_specific_predictions_dfs=[]\n",
    "    for subject in subjects: # leave one-subject out cross-validation:\n",
    "        without_subject_df=df1.copy().loc[df1.subject!=subject,:] # hold out one subject            \n",
    "        subject_specific_predictions=without_subject_df.groupby(['sentence1','sentence2','sentence_pair_id'])['chose_0'].mean() # mean choice probabilities of the remaining subjects        \n",
    "        subject_specific_predictions=subject_specific_predictions.reset_index().rename(columns={'chose_0':'choice_0_prob'})\n",
    "        subject_specific_predictions['choice_1_prob']=1.0-np.asarray(subject_specific_predictions.choice_0_prob)                \n",
    "        subject_specific_predictions.loc[:,'subject']=subject # we mark the resulting predictions as subject-specific predictions\n",
    "        \n",
    "        # next, we remove predictions for sentence pairs the subject didn't see:\n",
    "        cur_subject_df=df1.copy().loc[df1.subject==subject,:] \n",
    "        cur_subject_sentence_pair_ids=np.unique(cur_subject_df.sentence_pair_id) # these are the sentences the subject saw\n",
    "        subject_specific_predictions=subject_specific_predictions.loc[np.in1d(subject_specific_predictions.sentence_pair_id,cur_subject_sentence_pair_ids),:]                \n",
    "        subject_specific_predictions_dfs.append(subject_specific_predictions) # and add the remaining predictions to the list\n",
    "    \n",
    "    lb_NC_predictions=pd.concat(subject_specific_predictions_dfs)\n",
    "    \n",
    "    return lb_NC_predictions\n",
    "lb_NC_predictions=get_lb_noise_ceiling_predictions(protocol)\n",
    "    \n",
    "ub_NC_NLL=evaluate_NLL(protocol,ub_NC_predictions,mode='sum')\n",
    "print(\"upper noise ceiling NLL\",ub_NC_NLL)\n",
    "\n",
    "# # without a Gamma parameter, the lower noise ceiling can be infinity\n",
    "# lb_NC_NLL,df=evaluate_NLL(protocol,lb_NC_predictions,mode='sum',return_non_agg_NLL=True)\n",
    "# print(\"lower noise ceiling NLL\",lb_NC_NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "      <th>choice_0_prob</th>\n",
       "      <th>choice_1_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A agony it Rapids and the bikini dipping</td>\n",
       "      <td>Most are the names of arrogance native land</td>\n",
       "      <td>A agony it Rapids and the bikini dipping_Most ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A agony it Rapids and the bikini dipping</td>\n",
       "      <td>The Marina is a sailing boat from Scotland</td>\n",
       "      <td>A agony it Rapids and the bikini dipping_The M...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A and had to the past it again</td>\n",
       "      <td>The overpowering the lawn of such a year</td>\n",
       "      <td>A and had to the past it again_The overpowerin...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A case of driver replacements involved John Bowe</td>\n",
       "      <td>It includes notable people in the modern era</td>\n",
       "      <td>A case of driver replacements involved John Bo...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A civilization that was the largest weighing in</td>\n",
       "      <td>Talent beaches mirroring the publicly block tr...</td>\n",
       "      <td>A civilization that was the largest weighing i...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>This tarnished devising of seconded cruelty an...</td>\n",
       "      <td>To retroactively your team of style and workout</td>\n",
       "      <td>This tarnished devising of seconded cruelty an...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Too much separates relax fabric choral rudder ...</td>\n",
       "      <td>Use it for this and many other things</td>\n",
       "      <td>Too much separates relax fabric choral rudder ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Trust reflect slugging draped robots shoe firs...</td>\n",
       "      <td>Worth a turkey license issuance fortune choppe...</td>\n",
       "      <td>Trust reflect slugging draped robots shoe firs...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Was excited when I saw the awesome formatting</td>\n",
       "      <td>We were fools not to have known it</td>\n",
       "      <td>Was excited when I saw the awesome formatting_...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>We need to have some real banking reform</td>\n",
       "      <td>What you say may very well be true</td>\n",
       "      <td>We need to have some real banking reform_What ...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "0             A agony it Rapids and the bikini dipping   \n",
       "1             A agony it Rapids and the bikini dipping   \n",
       "2                       A and had to the past it again   \n",
       "3     A case of driver replacements involved John Bowe   \n",
       "4      A civilization that was the largest weighing in   \n",
       "..                                                 ...   \n",
       "600  This tarnished devising of seconded cruelty an...   \n",
       "601  Too much separates relax fabric choral rudder ...   \n",
       "602  Trust reflect slugging draped robots shoe firs...   \n",
       "603      Was excited when I saw the awesome formatting   \n",
       "604           We need to have some real banking reform   \n",
       "\n",
       "                                             sentence2  \\\n",
       "0          Most are the names of arrogance native land   \n",
       "1           The Marina is a sailing boat from Scotland   \n",
       "2             The overpowering the lawn of such a year   \n",
       "3         It includes notable people in the modern era   \n",
       "4    Talent beaches mirroring the publicly block tr...   \n",
       "..                                                 ...   \n",
       "600    To retroactively your team of style and workout   \n",
       "601              Use it for this and many other things   \n",
       "602  Worth a turkey license issuance fortune choppe...   \n",
       "603                 We were fools not to have known it   \n",
       "604                 What you say may very well be true   \n",
       "\n",
       "                                      sentence_pair_id  choice_0_prob  \\\n",
       "0    A agony it Rapids and the bikini dipping_Most ...           0.00   \n",
       "1    A agony it Rapids and the bikini dipping_The M...           0.00   \n",
       "2    A and had to the past it again_The overpowerin...           0.25   \n",
       "3    A case of driver replacements involved John Bo...           0.25   \n",
       "4    A civilization that was the largest weighing i...           0.75   \n",
       "..                                                 ...            ...   \n",
       "600  This tarnished devising of seconded cruelty an...           0.25   \n",
       "601  Too much separates relax fabric choral rudder ...           0.00   \n",
       "602  Trust reflect slugging draped robots shoe firs...           0.75   \n",
       "603  Was excited when I saw the awesome formatting_...           0.25   \n",
       "604  We need to have some real banking reform_What ...           0.20   \n",
       "\n",
       "     choice_1_prob  \n",
       "0             1.00  \n",
       "1             1.00  \n",
       "2             0.75  \n",
       "3             0.75  \n",
       "4             0.25  \n",
       "..             ...  \n",
       "600           0.75  \n",
       "601           1.00  \n",
       "602           0.25  \n",
       "603           0.75  \n",
       "604           0.80  \n",
       "\n",
       "[605 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub_NC_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo sentence log-probabilities\n",
    "In order to deal with the noise ceiling estimates in the same fashion we deal with the deep nets' outputs, we convert their choice probabilities pseudo-sentence probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_sentence_probs(p_choice_1,p_choice_2,pseudo_sentence_prob_gamma=1.234,eps=1e-8):\n",
    "    p_choice_1=np.clip(p_choice_1,eps,1-eps)\n",
    "    p_choice_2=np.clip(p_choice_2,eps,1-eps)\n",
    "    \n",
    "    s1a_b=-np.log((1-p_choice_1)/p_choice_1)*pseudo_sentence_prob_gamma  \n",
    "    lp1=-100+s1a_b/2\n",
    "    lp2=-100-s1a_b/2\n",
    "\n",
    "    return lp1,lp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['bigram','trigram','rnn','lstm','bilstm','bert','bert_whole_word','roberta','xlm','electra','gpt2','ub_NC','lb_NC']\n",
    "\n",
    "comp_models=list(np.setdiff1d(models,['ub_NC','lb_NC']))\n",
    "human_models=list(np.setdiff1d(models,comp_models))\n",
    "\n",
    "def get_cached_sentence_log_probabilities_for_model(model_name,protocol):\n",
    "    \"\"\" build a pandas dataframe of model raw sentence probabilities using cached model outputs\"\"\"\n",
    "    f = open(model_name+'_expt1_sentence_probs.pkl','rb')\n",
    "    prob_dict=pickle.load(f)\n",
    "    model_predictions=protocol[['sentence1','sentence2','sentence_pair_id']].drop_duplicates().reset_index(drop=True) \n",
    "    for i_row, row in model_predictions.iterrows():\n",
    "        model_predictions.at[i_row,'sentence1_raw_log_prob']=prob_dict[row['sentence1']]\n",
    "        model_predictions.at[i_row,'sentence2_raw_log_prob']=prob_dict[row['sentence2']]\n",
    "    return model_predictions\n",
    "\n",
    "def noise_ceiling_to_pseudo_log_probabilities(NC_predictions):\n",
    "    \"\"\" recast trial-specific noise ceiling predictions to be language-model like \"\"\"\n",
    "    NC_predictions=NC_predictions.copy()\n",
    "    p1=NC_predictions['choice_0_prob']\n",
    "    p2=NC_predictions['choice_1_prob']\n",
    "    assert np.isfinite(p1).all() and np.isfinite(p2).all(), 'found nan predictions for subject {}: {}).'.format(trial.subject,trial.sentence_pair_id)\n",
    "    log_p1,log_p2=pseudo_sentence_probs(p1,p2)\n",
    "    NC_predictions['sentence1_raw_log_prob']=log_p1\n",
    "    NC_predictions['sentence2_raw_log_prob']=log_p2\n",
    "    NC_predictions.drop(columns=['choice_0_prob','choice_1_prob'])\n",
    "    return NC_predictions\n",
    "\n",
    "model_sentence_predictions=OrderedDict() # this will hold a dataframe for each model\n",
    "\n",
    "for model_name in models:\n",
    "    if model_name in comp_models:\n",
    "        model_sentence_predictions[model_name]=get_cached_sentence_log_probabilities_for_model(model_name,protocol)\n",
    "    elif model_name=='ub_NC':\n",
    "        model_sentence_predictions[model_name]=noise_ceiling_to_pseudo_log_probabilities(ub_NC_predictions)\n",
    "    elif model_name=='lb_NC':\n",
    "        model_sentence_predictions[model_name]=noise_ceiling_to_pseudo_log_probabilities(lb_NC_predictions)\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence_pair_id</th>\n",
       "      <th>choice_0_prob</th>\n",
       "      <th>choice_1_prob</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentence1_raw_log_prob</th>\n",
       "      <th>sentence2_raw_log_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A naturally occurring replies punishment exami...</td>\n",
       "      <td>This country uses collaborative ping internal ...</td>\n",
       "      <td>A naturally occurring replies punishment exami...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-111.365560</td>\n",
       "      <td>-88.634440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A poets piped enquiry a lot of it</td>\n",
       "      <td>The growing spectral and thus attain the bandit</td>\n",
       "      <td>A poets piped enquiry a lot of it_The growing ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A poster and employer approval rating and vows</td>\n",
       "      <td>The next to the time and historical park</td>\n",
       "      <td>A poster and employer approval rating and vows...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.322156</td>\n",
       "      <td>-100.677844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Also their navy is in a dismal state</td>\n",
       "      <td>He had never told us he was married</td>\n",
       "      <td>Also their navy is in a dismal state_He had ne...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>And also as used in the plant printers</td>\n",
       "      <td>Hank rusted transcended youthful tiles manslau...</td>\n",
       "      <td>And also as used in the plant printers_Hank ru...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.322156</td>\n",
       "      <td>-100.677844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>The overall growth rate of the damned thing</td>\n",
       "      <td>There are notoriously marshes and fatally simu...</td>\n",
       "      <td>The overall growth rate of the damned thing_Th...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.572328</td>\n",
       "      <td>-100.427672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>There has been huge receptions at every gate</td>\n",
       "      <td>They were to go there on a coach</td>\n",
       "      <td>There has been huge receptions at every gate_T...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>23</td>\n",
       "      <td>-100.427672</td>\n",
       "      <td>-99.572328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>There is a chemical imbalance in our brains</td>\n",
       "      <td>Under Abbot it never has and never will</td>\n",
       "      <td>There is a chemical imbalance in our brains_Un...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>-88.634440</td>\n",
       "      <td>-111.365560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>These tables were lists of those who participate</td>\n",
       "      <td>Turnpike Gutierrez at newcomer golfers may ste...</td>\n",
       "      <td>These tables were lists of those who participa...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.322156</td>\n",
       "      <td>-100.677844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>They discuss the gambling stakes and she agrees</td>\n",
       "      <td>With Ross Smith and corners Shakespeare in London</td>\n",
       "      <td>They discuss the gambling stakes and she agree...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>23</td>\n",
       "      <td>-99.572328</td>\n",
       "      <td>-100.427672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "9    A naturally occurring replies punishment exami...   \n",
       "10                   A poets piped enquiry a lot of it   \n",
       "11      A poster and employer approval rating and vows   \n",
       "37                Also their navy is in a dismal state   \n",
       "46              And also as used in the plant printers   \n",
       "..                                                 ...   \n",
       "582        The overall growth rate of the damned thing   \n",
       "594       There has been huge receptions at every gate   \n",
       "595        There is a chemical imbalance in our brains   \n",
       "596   These tables were lists of those who participate   \n",
       "597    They discuss the gambling stakes and she agrees   \n",
       "\n",
       "                                             sentence2  \\\n",
       "9    This country uses collaborative ping internal ...   \n",
       "10     The growing spectral and thus attain the bandit   \n",
       "11            The next to the time and historical park   \n",
       "37                 He had never told us he was married   \n",
       "46   Hank rusted transcended youthful tiles manslau...   \n",
       "..                                                 ...   \n",
       "582  There are notoriously marshes and fatally simu...   \n",
       "594                   They were to go there on a coach   \n",
       "595            Under Abbot it never has and never will   \n",
       "596  Turnpike Gutierrez at newcomer golfers may ste...   \n",
       "597  With Ross Smith and corners Shakespeare in London   \n",
       "\n",
       "                                      sentence_pair_id  choice_0_prob  \\\n",
       "9    A naturally occurring replies punishment exami...       0.000000   \n",
       "10   A poets piped enquiry a lot of it_The growing ...       0.500000   \n",
       "11   A poster and employer approval rating and vows...       0.750000   \n",
       "37   Also their navy is in a dismal state_He had ne...       0.500000   \n",
       "46   And also as used in the plant printers_Hank ru...       0.750000   \n",
       "..                                                 ...            ...   \n",
       "582  The overall growth rate of the damned thing_Th...       0.666667   \n",
       "594  There has been huge receptions at every gate_T...       0.333333   \n",
       "595  There is a chemical imbalance in our brains_Un...       1.000000   \n",
       "596  These tables were lists of those who participa...       0.750000   \n",
       "597  They discuss the gambling stakes and she agree...       0.666667   \n",
       "\n",
       "     choice_1_prob  subject  sentence1_raw_log_prob  sentence2_raw_log_prob  \n",
       "9         1.000000        0             -111.365560              -88.634440  \n",
       "10        0.500000        0             -100.000000             -100.000000  \n",
       "11        0.250000        0              -99.322156             -100.677844  \n",
       "37        0.500000        0             -100.000000             -100.000000  \n",
       "46        0.250000        0              -99.322156             -100.677844  \n",
       "..             ...      ...                     ...                     ...  \n",
       "582       0.333333       23              -99.572328             -100.427672  \n",
       "594       0.666667       23             -100.427672              -99.572328  \n",
       "595       0.000000       23              -88.634440             -111.365560  \n",
       "596       0.250000       23              -99.322156             -100.677844  \n",
       "597       0.333333       23              -99.572328             -100.427672  \n",
       "\n",
       "[2570 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example:\n",
    "model_sentence_predictions['lb_NC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss: 1575.7269904816894\n",
      "converged in 10 epochs, 0.11017632484436035 seconds\n",
      "final loss  1575.7269904816894\n"
     ]
    }
   ],
   "source": [
    "def fit_model_decision_parameters(protocol,model_predictions,decision_model_class='FixedWidthSquashing',device='cpu',optimizer='LBFGS',minps=None,max_steps_without_improvement=10,return_dataframes=True,verbose=True):\n",
    "    \"\"\" fit the model decision hyper-parameter given a human data (protocol) and sentence predictions \n",
    "    \n",
    "    args:\n",
    "    protocol (pd.dataframe) must include a sentence_pair_id column\n",
    "    model_prediction (pd.dataframe) must include a sentence_pair_id column as well as sentence1_raw_log_prob and sentence2_raw_log_prob\n",
    "    decision_model_class (str) name of class to import from model_to_human_decision_torch. Currently implemented: 'Naive', 'NoSquashing', 'FixedWidthSquashing', 'VariableWidthSquashing', 'SquashedSoftmax'\n",
    "    device (str) torch device. cpu is faster for float64\n",
    "    optimizer (str) 'LBFGS' or 'Adam'\n",
    "    minps (float) initial values for the squashing parameter\n",
    "    max_steps_without_improvement (int) when to stop when not improving\n",
    "    return_dataframes (boolean) see below\n",
    "    verbose (boolean) show messages\n",
    "    \n",
    "    returns:\n",
    "    if return_dataframes is True:\n",
    "        decision_model (ModelToHumanDecision) a fitted decision model object\n",
    "        parameters_dict (dict) the fitted parameters\n",
    "        protocol_with_NLL (pd.dataframe) the protocol with additional NLL column (trial-specific NLL)\n",
    "        adjusted_model_predictions (pd.dataframe) the model predictions with additional columns for human choice probabilities\n",
    "    otherwise:\n",
    "        decision_model, parameters_dict        \n",
    "    \"\"\"    \n",
    "    \n",
    "    if 'subject' in model_predictions.columns:\n",
    "        identifiers=['subject','sentence1','sentence2','sentence_pair_id']\n",
    "    else:\n",
    "        identifiers=['sentence1','sentence2','sentence_pair_id']\n",
    "        \n",
    "    merged=protocol.merge(model_predictions,on=identifiers)  \n",
    "    \n",
    "    log_p1=torch.tensor(merged['sentence1_raw_log_prob'],device=device,dtype=torch.float64)\n",
    "    log_p2=torch.tensor(merged['sentence2_raw_log_prob'],device=device,dtype=torch.float64)\n",
    "    response_ind=torch.tensor(merged['Choice'],device=device,dtype=int)\n",
    "    \n",
    "    parameters={}\n",
    "    if decision_model_class in ['FixedWidthSquashing','VariableWidthSquashing']:\n",
    "        if minps is None: # get minimal sentence log-probability to serve as initial value for the squashing parameters\n",
    "            minps=abs(log_p1.mean()+log_p2.mean())/2\n",
    "        parameters['squashes']=minps\n",
    "    \n",
    "    # build decision model object\n",
    "    decision_model=getattr(model_to_human_decision_torch,decision_model_class)(parameters=parameters,device=device)\n",
    "    \n",
    "    for par in decision_model.parameters():\n",
    "        par.requires_grad=True\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        opt = optim.Adam(decision_model.parameters(),lr=1.0)\n",
    "    elif optimizer=='LBFGS':\n",
    "        opt = optim.LBFGS(decision_model.parameters(),line_search_fn='strong_wolfe') # this is much faster!\n",
    "    else:\n",
    "        raise ValueError\n",
    "      \n",
    "    previous_loss=torch.tensor(np.inf,device=device)\n",
    "   \n",
    "    eps=1e-10\n",
    "    non_improvement_counter=0\n",
    "    t=time.time()\n",
    "    for epoch in range(10000):\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        def calc_loss():\n",
    "            global my_loss\n",
    "            log_p_sent=decision_model(log_p1,log_p2)\n",
    "            take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "            log_p_choice=take_by_2nd_dim(log_p_sent,response_ind)\n",
    "            loss=log_p_choice.sum()     \n",
    "            return loss\n",
    "        \n",
    "        def closure(): # require for LBFGS\n",
    "            opt.zero_grad()\n",
    "            loss=calc_loss()\n",
    "            loss.backward()            \n",
    "            return loss\n",
    "    \n",
    "        opt.step(closure)\n",
    "    \n",
    "        loss=calc_loss().detach()\n",
    "        \n",
    "        if (previous_loss-loss)<eps:\n",
    "            non_improvement_counter+=1            \n",
    "        else:            \n",
    "            non_improvement_counter=0        \n",
    "        previous_loss=loss.detach()\n",
    "        \n",
    "        if non_improvement_counter>=max_steps_without_improvement:\n",
    "            if verbose:\n",
    "                print('converged in',epoch,'epochs,',time.time()-t,'seconds')\n",
    "            break\n",
    "            \n",
    "        if epoch%50==0 and verbose:            \n",
    "            print('epoch: ', epoch, 'loss:',loss.item())\n",
    "            # TODO: show parameters\n",
    "\n",
    "            #     print(decision_model.get_parameters())\n",
    "#     print('total loss:',loss)\n",
    "#     for i_model,model_name in enumerate(models):\n",
    "#         print('{} {:.2f}'.format(model_name,model_loss[i_model].item()))\n",
    "        \n",
    "    #breakpoint()\n",
    "    if verbose:\n",
    "        print(\"final loss \",loss.item())\n",
    "\n",
    "    parameters_dict=decision_model.get_parameters()\n",
    "    \n",
    "    NLL=loss.item()\n",
    "    \n",
    "    if return_dataframes:        \n",
    "        # trial-specific NLL\n",
    "        log_p_sent=decision_model(log_p1,log_p2)\n",
    "        take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "        log_p_choice=take_by_2nd_dim(log_p_sent,response_ind)\n",
    "            \n",
    "        protocol_with_NLL=protocol.copy()\n",
    "        protocol_with_NLL['NLL']=log_p_choice.detach().cpu().numpy()\n",
    "        adjusted_model_predictions=model_predictions.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            decision_log_prob=-decision_model(model_predictions.filter(regex='_raw_log_prob$',axis=1).to_numpy()).detach().cpu().numpy()\n",
    "        adjusted_model_predictions['choice_0_log_prob']=decision_log_prob[:,0]\n",
    "        adjusted_model_predictions['choice_1_log_prob']=decision_log_prob[:,1]\n",
    "        adjusted_model_predictions=adjusted_model_predictions.drop(columns=['sentence1_raw_log_prob','sentence2_raw_log_prob','choice_0_prob','choice_1_prob'],errors='ignore')        \n",
    "        return decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions\n",
    "    else:\n",
    "        return decision_model, parameters_dict, NLL\n",
    "\n",
    "decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions['bert_whole_word'],decision_model_class='FixedWidthSquashing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sanity checks:\n",
    "assert np.isclose(NLL,evaluate_NLL(protocol,adjusted_model_predictions,mode='sum'))\n",
    "assert np.isclose(NLL,protocol_with_NLL['NLL'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss: 1560.3624454803753\n",
      "converged in 10 epochs, 0.09047508239746094 seconds\n",
      "final loss  1560.3624454803753\n",
      "{'gamma': 22.289680575705994, 'eta': -7.2758713340352985}\n",
      "epoch:  0 loss: 1566.7094030882274\n",
      "converged in 10 epochs, 0.09165120124816895 seconds\n",
      "final loss  1566.7094030882274\n",
      "{'gamma': 27.878253397666732, 'eta': -6.530540765020858}\n",
      "epoch:  0 loss: 1569.661343386043\n",
      "converged in 10 epochs, 0.09599471092224121 seconds\n",
      "final loss  1569.661343386043\n",
      "{'gamma': 23.767387064961653, 'eta': -3.7020886214367996}\n",
      "epoch:  0 loss: 1555.1082232327806\n",
      "converged in 10 epochs, 0.08859562873840332 seconds\n",
      "final loss  1555.1082232327806\n",
      "{'gamma': 24.102958156654907, 'eta': -3.663969531817954}\n",
      "epoch:  0 loss: 1564.224262665215\n",
      "converged in 10 epochs, 0.0970468521118164 seconds\n",
      "final loss  1564.224262665215\n",
      "{'gamma': 22.976203485153608, 'eta': -4.039183709002762}\n",
      "epoch:  0 loss: 1571.4023663344628\n",
      "converged in 10 epochs, 0.06435561180114746 seconds\n",
      "final loss  1571.4023663344628\n",
      "{'gamma': 36.37445721744355, 'eta': -119.41588595552977}\n",
      "epoch:  0 loss: 1568.8309115091536\n",
      "converged in 10 epochs, 0.0931549072265625 seconds\n",
      "final loss  1568.8309115091536\n",
      "{'gamma': 26.37767679160392, 'eta': -3.852920110005497}\n",
      "epoch:  0 loss: 1562.306885159803\n",
      "converged in 10 epochs, 0.06091642379760742 seconds\n",
      "final loss  1562.306885159803\n",
      "{'gamma': 36.164763504347, 'eta': -115.73933240531021}\n",
      "epoch:  0 loss: 1566.5096281340257\n",
      "converged in 10 epochs, 0.08053350448608398 seconds\n",
      "final loss  1566.5096281340257\n",
      "{'gamma': 25.415566081372326, 'eta': -3.9150757688658557}\n",
      "epoch:  0 loss: 1553.3221071841576\n",
      "converged in 10 epochs, 0.07792377471923828 seconds\n",
      "final loss  1553.3221071841576\n",
      "{'gamma': 22.709086862103227, 'eta': -3.9010947968494}\n",
      "epoch:  0 loss: 1554.8662050936528\n",
      "converged in 10 epochs, 0.07427668571472168 seconds\n",
      "final loss  1554.8662050936528\n",
      "{'gamma': 23.710815051465662, 'eta': -4.027733813158163}\n",
      "epoch:  0 loss: 1249.0232759349947\n",
      "converged in 13 epochs, 0.3716545104980469 seconds\n",
      "final loss  1142.90236847332\n",
      "{'gamma': 1.233999462127061, 'eta': -98.71386054506952}\n",
      "epoch:  0 loss: 1681.7385610192114\n",
      "converged in 11 epochs, 0.12698721885681152 seconds\n",
      "final loss  1681.7384208953658\n",
      "{'gamma': 15.571235131737332, 'eta': -7.141626309734798}\n"
     ]
    }
   ],
   "source": [
    "# train and save decision models for all models\n",
    "decision_model_class='SquashedSoftmax'\n",
    "optimizer='LBFGS'\n",
    "# train and save models\n",
    "for model in models:\n",
    "    decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions[model],decision_model_class=decision_model_class,optimizer=optimizer,verbose=True)\n",
    "    path = os.path.join('decision_models','20201228',decision_model_class+\"_\" +optimizer,model+'.pkl')\n",
    "    print(parameters_dict)\n",
    "    decision_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different response models and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam NoSquashing bigram  1562.3192279513883\n",
      "Adam NoSquashing trigram  1566.991181765924\n",
      "Adam NoSquashing rnn  1587.3529176545892\n",
      "Adam NoSquashing lstm  1572.4363987480233\n",
      "Adam NoSquashing bilstm  1570.516391718862\n",
      "Adam NoSquashing bert  1571.4023663346175\n",
      "Adam NoSquashing bert_whole_word  1572.7144879146963\n",
      "Adam NoSquashing roberta  1562.3068851602252\n",
      "Adam NoSquashing xlm  1570.7725019707411\n",
      "Adam NoSquashing electra  1558.4663326072423\n",
      "Adam NoSquashing gpt2  1559.117165418958\n",
      "Adam NoSquashing ub_NC  1142.9023684719257\n",
      "Adam NoSquashing lb_NC  1682.1092870661735\n",
      "Adam FixedWidthSquashing bigram  1562.3191215939742\n",
      "Adam FixedWidthSquashing trigram  1566.991186644725\n",
      "Adam FixedWidthSquashing rnn  1571.7694177123399\n",
      "Adam FixedWidthSquashing lstm  1562.91812837726\n",
      "Adam FixedWidthSquashing bilstm  1569.9526168772256\n",
      "Adam FixedWidthSquashing bert  1571.2093618570177\n",
      "Adam FixedWidthSquashing bert_whole_word  1575.7269904829268\n",
      "Adam FixedWidthSquashing roberta  1561.2285012673174\n",
      "Adam FixedWidthSquashing xlm  1570.4252271461446\n",
      "Adam FixedWidthSquashing electra  1560.6787931588249\n",
      "Adam FixedWidthSquashing gpt2  1557.893636933542\n",
      "Adam FixedWidthSquashing ub_NC  1142.9023691807542\n",
      "Adam FixedWidthSquashing lb_NC  1679.2063105645343\n",
      "Adam VariableWidthSquashing bigram  1560.0818286120239\n",
      "Adam VariableWidthSquashing trigram  1566.3684142400082\n",
      "Adam VariableWidthSquashing rnn  1569.6474826779481\n",
      "Adam VariableWidthSquashing lstm  1554.3125728377206\n",
      "Adam VariableWidthSquashing bilstm  1564.3399269861934\n",
      "Adam VariableWidthSquashing bert  1565.3424551578332\n",
      "Adam VariableWidthSquashing bert_whole_word  1568.4866286472202\n",
      "Adam VariableWidthSquashing roberta  1552.6002279373129\n",
      "Adam VariableWidthSquashing xlm  1565.607264501071\n",
      "Adam VariableWidthSquashing electra  1554.3583231458226\n",
      "Adam VariableWidthSquashing gpt2  1556.0033195099213\n",
      "Adam VariableWidthSquashing ub_NC  1142.9023684719484\n",
      "Adam VariableWidthSquashing lb_NC  1677.5740682402586\n",
      "Adam SquashedSoftmax bigram  1562.8907350982993\n",
      "Adam SquashedSoftmax trigram  1567.590838278515\n",
      "Adam SquashedSoftmax rnn  1569.6613433860816\n",
      "Adam SquashedSoftmax lstm  1555.108223232789\n",
      "Adam SquashedSoftmax bilstm  1564.224262665389\n",
      "Adam SquashedSoftmax bert  1565.5173837409827\n",
      "Adam SquashedSoftmax bert_whole_word  1568.830911508856\n",
      "Adam SquashedSoftmax roberta  1553.8485733491952\n",
      "Adam SquashedSoftmax xlm  1566.5096281343126\n",
      "Adam SquashedSoftmax electra  1553.3221071841715\n",
      "Adam SquashedSoftmax gpt2  1554.86620509375\n",
      "Adam SquashedSoftmax ub_NC  1294.0076039710511\n",
      "Adam SquashedSoftmax lb_NC  1682.5068126566482\n",
      "LBFGS NoSquashing bigram  1562.3191163859792\n",
      "LBFGS NoSquashing trigram  1566.9911817659076\n",
      "LBFGS NoSquashing rnn  1587.3529176526847\n",
      "LBFGS NoSquashing lstm  1572.4363987461275\n",
      "LBFGS NoSquashing bilstm  1570.516391718296\n",
      "LBFGS NoSquashing bert  1571.4023663341602\n",
      "LBFGS NoSquashing bert_whole_word  1572.7144879139698\n",
      "LBFGS NoSquashing roberta  1562.3068851598036\n",
      "LBFGS NoSquashing xlm  1570.7725019701245\n",
      "LBFGS NoSquashing electra  1558.4663326069217\n",
      "LBFGS NoSquashing gpt2  1559.1171654186808\n",
      "LBFGS NoSquashing ub_NC  1142.9023684719082\n",
      "LBFGS NoSquashing lb_NC  1681.7886618572898\n",
      "LBFGS FixedWidthSquashing bigram  1562.3191163868746\n",
      "LBFGS FixedWidthSquashing trigram  1566.9911818370085\n",
      "LBFGS FixedWidthSquashing rnn  1571.7694177124622\n",
      "LBFGS FixedWidthSquashing lstm  1562.918128376879\n",
      "LBFGS FixedWidthSquashing bilstm  1569.9526136741895\n",
      "LBFGS FixedWidthSquashing bert  1571.207543638578\n",
      "LBFGS FixedWidthSquashing bert_whole_word  1575.7269904816894\n",
      "LBFGS FixedWidthSquashing roberta  1561.228403569909\n",
      "LBFGS FixedWidthSquashing xlm  1570.4252271452651\n",
      "LBFGS FixedWidthSquashing electra  1560.6782868610585\n",
      "LBFGS FixedWidthSquashing gpt2  1557.8934938366726\n",
      "LBFGS FixedWidthSquashing ub_NC  1142.902368473316\n",
      "LBFGS FixedWidthSquashing lb_NC  1679.2063105645072\n",
      "LBFGS VariableWidthSquashing bigram  1559.5090047936758\n",
      "LBFGS VariableWidthSquashing trigram  1566.112010446681\n",
      "LBFGS VariableWidthSquashing rnn  1569.1553408121513\n",
      "LBFGS VariableWidthSquashing lstm  1552.1261481464749\n",
      "LBFGS VariableWidthSquashing bilstm  1562.9442361827214\n",
      "LBFGS VariableWidthSquashing bert  1563.6135517896314\n",
      "LBFGS VariableWidthSquashing bert_whole_word  1566.475656090925\n",
      "LBFGS VariableWidthSquashing roberta  1551.0245758071228\n",
      "LBFGS VariableWidthSquashing xlm  1564.7696416057543\n",
      "LBFGS VariableWidthSquashing electra  1550.3686059808058\n",
      "LBFGS VariableWidthSquashing gpt2  1552.4572749715103\n",
      "LBFGS VariableWidthSquashing ub_NC  1142.9024598053866\n",
      "LBFGS VariableWidthSquashing lb_NC  1677.572069473253\n",
      "LBFGS SquashedSoftmax bigram  1560.3624454803753\n",
      "LBFGS SquashedSoftmax trigram  1566.7094030882274\n",
      "LBFGS SquashedSoftmax rnn  1569.661343386043\n",
      "LBFGS SquashedSoftmax lstm  1555.1082232327806\n",
      "LBFGS SquashedSoftmax bilstm  1564.224262665215\n",
      "LBFGS SquashedSoftmax bert  1571.4023663344628\n",
      "LBFGS SquashedSoftmax bert_whole_word  1568.8309115091536\n",
      "LBFGS SquashedSoftmax roberta  1562.306885159803\n",
      "LBFGS SquashedSoftmax xlm  1566.5096281340257\n",
      "LBFGS SquashedSoftmax electra  1553.3221071841576\n",
      "LBFGS SquashedSoftmax gpt2  1554.8662050936528\n",
      "LBFGS SquashedSoftmax ub_NC  1142.90236847332\n",
      "LBFGS SquashedSoftmax lb_NC  1681.7384208953658\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for optimizer in ['Adam','LBFGS']:\n",
    "    for decision_model_class in ['NoSquashing', 'FixedWidthSquashing', 'VariableWidthSquashing', 'SquashedSoftmax']:\n",
    "        for model in models:\n",
    "            print (optimizer,decision_model_class,model,' ',end='')\n",
    "            decision_model, parameters_dict, NLL, protocol_with_NLL, adjusted_model_predictions=fit_model_decision_parameters(protocol,model_sentence_predictions[model],decision_model_class=decision_model_class,optimizer=optimizer,verbose=False)\n",
    "            assert np.isclose(NLL,evaluate_NLL(protocol,adjusted_model_predictions,mode='sum'))\n",
    "            assert np.isclose(NLL,protocol_with_NLL['NLL'].sum())\n",
    "            print(NLL)\n",
    "            results.append({'model':model,'decision_model_class':decision_model_class,'optimizer':optimizer,'NLL':NLL})\n",
    "results=pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.FacetGrid(results, row=\"model\")\n",
    "g.map(sns.barplot,'optimizer','NLL','decision_model_class')\n",
    "plt.ylim([800,900])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing data for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comp_models=list(np.setdiff1d(models,['ub_NC','lb_NC']))\n",
    "human_models=list(np.setdiff1d(models,comp_models))\n",
    "\n",
    "prob_dict=dict()\n",
    "for model_name in comp_models:\n",
    "    f = open(model_name+'_expt1_sentence_probs.pkl','rb')\n",
    "    prob_dict[model_name]=pickle.load(f)\n",
    "\n",
    "fitsets=[]\n",
    "for i_subject in range(n_subjects):\n",
    "    cur_sub_protocol=protocol.loc[protocol.subject==i_subject,:]\n",
    "    \n",
    "    fitset=[]\n",
    "    for i_trial, trial in cur_sub_protocol.iterrows():\n",
    "        for model_name in models:\n",
    "            if model_name in comp_models:\n",
    "                log_p1=prob_dict[model_name][trial.sentence1]\n",
    "                log_p2=prob_dict[model_name][trial.sentence2]\n",
    "                fitset.append([model_name,log_p1,log_p2,trial.Choice])\n",
    "            if model_name in human_models:\n",
    "                # get predictions from dataframe \n",
    "                predictions_df={'ub_NC':ub_NC_predictions,'lb_NC':lb_NC_predictions}[model_name]                \n",
    "                if 'subject' in predictions_df.columns: # deal with subject-specific predictions\n",
    "                    filter_vars=['sentence_pair_id','subject']\n",
    "                else:\n",
    "                    filter_vars=['sentence_pair_id']\n",
    "                    \n",
    "                cur_prediction=trial.to_frame().T.merge(predictions_df,how='left',on=filter_vars)\n",
    "                assert len(cur_prediction)==1, 'exactly one prediction should be provided for each trial (found {} for subject {}: {}).'.format(len(cur_prediction),trial.subject,trial.sentence_pair_id)\n",
    "                                    \n",
    "                p1=cur_prediction.loc[0,'choice_0_prob']\n",
    "                p2=cur_prediction.loc[0,'choice_1_prob']\n",
    "                \n",
    "                assert np.isfinite(p1) and np.isfinite(p2), 'found nan predictions for subject {}: {}).'.format(trial.subject,trial.sentence_pair_id)\n",
    "                \n",
    "                log_p1,log_p2=pseudo_sentence_probs(p1,p2)\n",
    "                fitset.append([model_name,log_p1,log_p2,trial.Choice])\n",
    "            \n",
    "    fitsets.append(fitset)\n",
    "\n",
    "fitsets_all=[]\n",
    "for fitset in fitsets:\n",
    "    fitsets_all+=fitset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model-specific ranges of sentence probabilities (used to initiate squash parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minps=[]\n",
    "for m in models:\n",
    "    m=np.min([np.min([trial[1],trial[2]]) for trial in fitsets_all if trial[0]==m])\n",
    "    minps.append(m)\n",
    "    \n",
    "minps=np.abs(minps)\n",
    "\n",
    "\n",
    "maxps=[]\n",
    "for m in models:\n",
    "    m=np.max([np.max([trial[1],trial[2]]) for trial in fitsets_all if trial[0]==m])\n",
    "    maxps.append(m)\n",
    "    \n",
    "maxps=np.abs(maxps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit human-decision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# possible decision models implemented: 'Naive', 'NoSquashing', 'FixedWidthSquashing', 'VariableWidthSquashing', 'SquashedSoftmax'\n",
    "decision_model_class='FixedWidthSquashing'\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "for boot in range(1):\n",
    "    \n",
    "    if boot == 0: # original order\n",
    "        boots=np.arange(n_subjects)\n",
    "    else: # bootstrap subjects\n",
    "        boots=np.random.choice(n_subjects,n_subjects)\n",
    "    \n",
    "    fitset=[]\n",
    "    for b in boots:\n",
    "        fitset+=fitsets[b]\n",
    "    \n",
    "    trainable_parameters_across_models=[]\n",
    "    \n",
    "    # define a human decision model for each model\n",
    "    decision_model=OrderedDict()\n",
    "    for i_model, model in enumerate(models):\n",
    "        parameters={}\n",
    "        if decision_model_class in ['FixedWidthSquashing','VariableWidthSquashing']:\n",
    "            parameters['squashes']=minps[i_model] # use minimal sentence probability as the initial value for squashing parameter\n",
    "        model_class=getattr(model_to_human_decision_torch,decision_model_class)\n",
    "        decision_model[model]=model_class(parameters=parameters,device=device)\n",
    "        for par in decision_model[model].parameters():\n",
    "            par.requires_grad=True\n",
    "        trainable_parameters_across_models.extend(decision_model[model].parameters())\n",
    "        \n",
    "    opt = optim.Adam(trainable_parameters_across_models,lr=1)\n",
    "\n",
    "    print(boot)\n",
    "    print('')\n",
    "\n",
    "    # build models by trials by sentences log-prob matrix\n",
    "    \n",
    "    log_p1=defaultdict(list)\n",
    "    log_p2=defaultdict(list)\n",
    "    response_ind=defaultdict(list)\n",
    "    \n",
    "    for trial in fitset:\n",
    "        model1_name=trial[0]\n",
    "        log_p1[model1_name].append(trial[1])\n",
    "        log_p2[model1_name].append(trial[2])\n",
    "        response_ind[model1_name].append(trial[3])        \n",
    "    \n",
    "    for epoch in range(1000): # for some models, one needs more than 200 epochs for convergence. TODO - replace this loop with a convergence criterion.\n",
    "\n",
    "        loss = torch.tensor(0.,device=device,requires_grad = True)\n",
    "        \n",
    "        model_loss=torch.zeros((len(models)),device=device)\n",
    "        \n",
    "        for i_model,model_name in enumerate(models):\n",
    "            log_p_sent=decision_model[model_name](log_p1[model1_name],log_p2[model1_name])\n",
    "            take_by_2nd_dim=lambda x, idx: x[torch.arange(x.size(0)), idx] \n",
    "            log_p_choice=take_by_2nd_dim(log_p_sent,response_ind[model1_name])\n",
    "            model_loss[i_model]=log_p_choice.sum()\n",
    "            loss=loss+model_loss[i_model]\n",
    "            \n",
    "        if epoch%50==0:\n",
    "            print('loss:',loss)\n",
    "#             if decision_model_class in ['FixedWidthSquashing','VariableWidthSquashing']:\n",
    "#                 squashes=decision_model\n",
    "#                 print ('squashes: ',end='')\n",
    "        \n",
    "#             for par_name, par in decision_model.parameters().items():\n",
    "#                 print(par_name,par)\n",
    "#             print('')\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    print(decision_model.get_parameters())\n",
    "    print('total loss:',loss)\n",
    "    for i_model,model_name in enumerate(models):\n",
    "        print('{} {:.2f}'.format(model_name,model_loss[i_model].item()))\n",
    "        \n",
    "    squashes=list(decision_model.parameters['squashes'].data.numpy())\n",
    "    gammas=list(decision_model.parameters['gamma'].data.numpy())\n",
    "    #gammas=float(decision_model.parameters['gamma'])\n",
    "    \n",
    "    if boot==0:\n",
    "        squash_boots=[squashes]\n",
    "        gamma_boots=[gammas]\n",
    "        losses=[m.item() for m in model_loss]\n",
    "    else:\n",
    "        squash_boots.append(squashes)\n",
    "        gamma_boots.append(gammas)\n",
    "        losses.append([m.item() for m in model_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_parameters_across_models(decision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gT=np.array(gamma_boots).T\n",
    "sT=np.array(squash_boots).T\n",
    "\n",
    "ccs=[]\n",
    "for i in range(11):\n",
    "    \n",
    "    cc=np.corrcoef(sT[i,:],gT[i,:])[0,1]\n",
    "    ccs.append(cc)\n",
    "    \n",
    "ccs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synthetic\n",
    "\n",
    "# tensor(671.4779, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.5626, 128.7844,  84.1193,  92.4041, 111.8108, 105.3416,  83.8154,\n",
    "#         116.5818,  87.0363,  60.6492,  81.0560], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([29.8585, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "#natural\n",
    "\n",
    "# tensor(148.7837, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([115.8415,  84.2844,  72.0876,  43.8699,  54.9998,  50.4821,  69.6983,\n",
    "#          49.1065,  42.4528,  34.6840,  59.5816], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([ 9.1418, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n",
    "\n",
    "\n",
    "# tensor(829.6835, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
    "# tensor([129.1968, 127.7177,  83.4297,  91.0015, 111.4391, 104.3089,  83.3092,\n",
    "#         115.3282,  85.6379,  59.8025,  79.7266], device='cuda:0',\n",
    "#        dtype=torch.float64, requires_grad=True)\n",
    "# tensor([28.2664, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
    "#         10.0000, 10.0000, 10.0000], device='cuda:0', requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squash_boots=np.array([[136.6031, 130.6315,  88.2857, 127.1014, 124.6718, 110.4878,  84.6417,\n",
    "#         118.1435, 107.7737,  63.0790,  79.9143],\n",
    "#               [137.9003, 125.7105,  84.5635, 127.1772, 112.0871,  94.5774,  78.6344,\n",
    "#         114.0863,  95.4273,  58.2541,  79.6323],\n",
    "#               [131.8761, 133.7181, 147.7142, 143.2530, 110.6293, 134.2203,  83.7578,\n",
    "#         139.0220,  82.7407,  66.7261,  79.2919],\n",
    "#               [139.6443, 135.6945,  80.5810,  94.8058, 116.6113, 103.0531,  87.4392,\n",
    "#         119.7775,  81.2322,  60.9193,  81.1773],\n",
    "#               [132.4967, 128.7608,  92.4549,  91.8489, 110.4346, 104.1544,  80.5317,\n",
    "#         126.0785, 124.9601,  57.4880,  76.9285],\n",
    "#               [128.2988, 112.9426,  80.0811,  90.9844, 111.0652, 115.8396,  56.3730,\n",
    "#          78.9889, 120.2621,  29.6331,  61.9582],\n",
    "#               [127.7181, 132.9688, 148.8201,  94.7272, 109.7225,  87.5404,  81.3132,\n",
    "#         115.8013, 116.9241,  64.7583,  73.5550],\n",
    "#               [120.8540, 116.8421, 135.4461,  88.5892, 124.1079, 133.7935,  86.1704,\n",
    "#         100.1683,  92.7386,  49.7565,  79.4477],\n",
    "#               [125.4391, 114.5180,  82.5459,  87.4416, 110.8198,  93.8739,  63.4568,\n",
    "#         117.9814,  83.9064,  57.2624,  77.2107],\n",
    "#               [129.6447, 129.3590,  86.6444,  92.5276, 109.5141, 105.0256,  65.8152,\n",
    "#         117.5682, 125.8277,  59.4612,  77.8789]])\n",
    "\n",
    "s=squash_boots\n",
    "\n",
    "squash_boots=np.array(losses).T\n",
    "squash_boots.sort()\n",
    "squash_boots=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squash_boots=squash_boots.reshape([10,11])\n",
    "\n",
    "\n",
    "\n",
    "x=np.arange(11)*2\n",
    "# plt.bar(x-0.2, maxps, width=0.1, color=[1,0,0])\n",
    "plt.bar(x-0.1, squash_boots[0], width=0.1, color=[0,0,1])\n",
    "plt.bar(x, squash_boots[1], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.1, squash_boots[2], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.2, squash_boots[3], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.3, squash_boots[4], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.4, squash_boots[5], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.5, squash_boots[6], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.6, squash_boots[7], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.7, squash_boots[8], width=0.1, color=[0,0,1])\n",
    "plt.bar(x+0.8, squash_boots[9], width=0.1, color=[0,0,1])\n",
    "# plt.bar(x+0.9, minps, width=0.1, color=[1,0,0])\n",
    "\n",
    "# plt.ylabel('Squash Thresholds')\n",
    "plt.ylabel('Negative log likelihood')\n",
    "plt.xticks(x,models,rotation=70)\n",
    "\n",
    "# plt.ylim([800,950])\n",
    "\n",
    "legs=['Min/Max baseline','Squash thresholds']\n",
    "# plt.legend(legs,bbox_to_anchor=(1, 0.3, 0.2, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=squash_boots.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=np.array([f.sort() for f in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_boots=d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist=list(set(all_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('test_sents_expt1_first13subs.txt','w')\n",
    "for s in slist:\n",
    "    file.write(s)\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_model.parameters['squashes'].data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sent1,sent2].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=0\n",
    "a=0\n",
    "for trial in fitset:\n",
    "    \n",
    "    if trial[0]=='electra':\n",
    "        \n",
    "        a+=1\n",
    "    \n",
    "        pa=trial[1]\n",
    "        pb=trial[2]\n",
    "        c=trial[3]\n",
    "\n",
    "        if c==0 and pa>pb:\n",
    "            g+=1\n",
    "        elif c==1 and pb>pa:\n",
    "            g+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g/len(fitset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(item_prob_ceilings[item_id][1])==-math.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
