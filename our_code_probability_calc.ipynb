{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate log probability of a sentence\n",
    "def calculate_log_probability(sentence):\n",
    "    # Tokenize the sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate output probabilities from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs[0]\n",
    "    \n",
    "    # Calculate log probability of each token\n",
    "    token_log_probs = torch.log_softmax(logits[0], dim=-1)\n",
    "    \n",
    "    # Get token IDs of the input sentence\n",
    "    input_token_ids = input_ids[0]\n",
    "    \n",
    "    # Sum up log probabilities of tokens in the input sentence\n",
    "    log_prob_sum = 0\n",
    "    for i, token_id in enumerate(input_token_ids):\n",
    "        log_prob_sum += token_log_probs[i, token_id]\n",
    "    \n",
    "    return log_prob_sum.item()\n",
    "\n",
    "# Example usage\n",
    "sentence = \"A message has been sent to your account\"\n",
    "log_probability = calculate_log_probability(sentence)\n",
    "print(\"Log probability of the sentence:\", log_probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article function\n",
    "\n",
    "(env : mamba install -c conda-forge ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_functions import model_factory\n",
    "model = model_factory(name = 'gpt2', gpu_id = None)\n",
    "sentence = \"A message has been sent to your account\"\n",
    "log_probability = model.sent_prob(sentence)\n",
    "print(\"Log probability of the sentence:\", log_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_functions import model_factory\n",
    "model = model_factory(name = 'gpt2', gpu_id = None)\n",
    "sentence = \"A message has been sent to your account\"\n",
    "log_probability = model.sent_prob(sentence)\n",
    "print(\"Log probability of the sentence:\", log_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handmade version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTContainer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(self.device)\n",
    "        self.starts = []\n",
    "        self.suffs = []\n",
    "\n",
    "        # Populate starts and suffs\n",
    "        for i in range(len(self.tokenizer.get_vocab())):\n",
    "            tok = self.tokenizer.decode(i)\n",
    "            if tok[0] == \" \" or tok[0] == \".\":\n",
    "                self.starts.append(i)\n",
    "            elif tok[0] != \" \":\n",
    "                self.suffs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = GPTContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "def gpt2_sent_prob(sent):\n",
    "\n",
    "    tokenizer = container.tokenizer\n",
    "    model = container.model\n",
    "\n",
    "    starts = container.starts\n",
    "    suffs = container.suffs\n",
    "\n",
    "    sent = \". \" + sent + \".\"\n",
    "\n",
    "    tokens = tokenizer.encode(sent)\n",
    "    inputs = torch.tensor(tokens).to(container.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(inputs)\n",
    "\n",
    "    unsoft = out[0]\n",
    "    lab1 = inputs.cpu().data.numpy()\n",
    "\n",
    "    probs = []\n",
    "    for x in range(len(lab1) - 1):\n",
    "\n",
    "        lab = lab1[x + 1]\n",
    "        unsoft1 = unsoft[x]\n",
    "\n",
    "        if lab in starts:\n",
    "\n",
    "            soft = logsoftmax(unsoft1[starts])\n",
    "            prob = float(soft[starts.index(lab)].cpu().data.numpy())\n",
    "\n",
    "        elif lab in suffs:\n",
    "\n",
    "            soft = logsoftmax(unsoft1[suffs])\n",
    "            prob = float(soft[suffs.index(lab)].cpu().data.numpy())\n",
    "\n",
    "        probs.append(prob)\n",
    "\n",
    "    prob = np.sum(probs)\n",
    "\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A message has been sent to your account\"\n",
    "log_probability = gpt2_sent_prob(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.383871644735336\n"
     ]
    }
   ],
   "source": [
    "print(log_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Githubcode\n",
    "\n",
    "https://gist.github.com/yuchenlin/eb63e2d0513f70cfc9bb85fa5a78953b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    " \n",
    "def model_init(model_string, cuda):\n",
    "    if model_string.startswith(\"gpt2\"):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_string)\n",
    "    else:\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n",
    "        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.to('cuda')\n",
    "    print(\"Model init\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def sent_scoring(model_tokenizer, text, cuda):\n",
    "    model = model_tokenizer[0]\n",
    "    tokenizer = model_tokenizer[1]\n",
    "    assert model is not None\n",
    "    assert tokenizer is not None\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    if cuda:\n",
    "        input_ids = input_ids.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    sentence_prob = loss.item()\n",
    "    return sentence_prob\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # model, tokenizer = model_init('openai-gpt', False) \n",
    "    model, tokenizer = model_init('gpt2', False) \n",
    "    print(sent_scoring((model, tokenizer), \"A message has been sent to your account\", False))\n",
    "    print(sent_scoring((model, tokenizer), \"They are barely able to handle Delhi properly\", False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface code\n",
    "\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"A message has been sent to your account\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = gpt2.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True)\n",
    "\n",
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
