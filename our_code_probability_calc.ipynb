{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFGPT2LMHeadModel' from 'transformers' (c:\\Users\\danie\\anaconda3\\envs\\contstimlang\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19756\\434332045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"openai-community/gpt2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"openai-community/gpt2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TFGPT2LMHeadModel' from 'transformers' (c:\\Users\\danie\\anaconda3\\envs\\contstimlang\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.53MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.15MB/s]\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 654kB/s]\n",
      "Downloading: 100%|██████████| 548M/548M [00:51<00:00, 10.6MB/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19504\\1082636230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This is a test sentence.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mlog_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_log_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log probability of the sentence:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19504\\1082636230.py\u001b[0m in \u001b[0;36mcalculate_log_probability\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Calculate log probability of each token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of the sentence: -66.4273452758789\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate log probability of a sentence\n",
    "def calculate_log_probability(sentence):\n",
    "    # Tokenize the sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate output probabilities from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs[0]\n",
    "    \n",
    "    # Calculate log probability of each token\n",
    "    token_log_probs = torch.log_softmax(logits[0], dim=-1)\n",
    "    \n",
    "    # Get token IDs of the input sentence\n",
    "    input_token_ids = input_ids[0]\n",
    "    \n",
    "    # Sum up log probabilities of tokens in the input sentence\n",
    "    log_prob_sum = 0\n",
    "    for i, token_id in enumerate(input_token_ids):\n",
    "        log_prob_sum += token_log_probs[i, token_id]\n",
    "    \n",
    "    return log_prob_sum.item()\n",
    "\n",
    "# Example usage\n",
    "sentence = \"A message has been sent to your account\"\n",
    "log_probability = calculate_log_probability(sentence)\n",
    "print(\"Log probability of the sentence:\", log_probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Githubcode\n",
    "\n",
    "https://gist.github.com/yuchenlin/eb63e2d0513f70cfc9bb85fa5a78953b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model init\n",
      "3.4872491359710693\n",
      "5.940968036651611\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    " \n",
    "def model_init(model_string, cuda):\n",
    "    if model_string.startswith(\"gpt2\"):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_string)\n",
    "    else:\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n",
    "        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.to('cuda')\n",
    "    print(\"Model init\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def sent_scoring(model_tokenizer, text, cuda):\n",
    "    model = model_tokenizer[0]\n",
    "    tokenizer = model_tokenizer[1]\n",
    "    assert model is not None\n",
    "    assert tokenizer is not None\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    if cuda:\n",
    "        input_ids = input_ids.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    sentence_prob = loss.item()\n",
    "    return sentence_prob\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # model, tokenizer = model_init('openai-gpt', False) \n",
    "    model, tokenizer = model_init('gpt2', False) \n",
    "    print(sent_scoring((model, tokenizer), \"A message has been sent to your account\", False))\n",
    "    print(sent_scoring((model, tokenizer), \"They are barely able to handle Delhi properly\", False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface code\n",
    "\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForCausalLM' from 'transformers' (c:\\Users\\danie\\anaconda3\\envs\\contstimlang\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19504\\178256946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AutoModelForCausalLM' from 'transformers' (c:\\Users\\danie\\anaconda3\\envs\\contstimlang\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"A message has been sent to your account\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = gpt2.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True)\n",
    "\n",
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contstimlang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
